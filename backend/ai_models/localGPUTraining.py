"""
Local GPU Training System
S·ª≠ d·ª•ng GPU tr√™n m√°y t√≠nh local ƒë·ªÉ train AiESP
T·ª± ƒë·ªông t√¨m t√†i li·ªáu v√† h·ªçc h·ªèi t·ª´ internet
"""

import json
import sys
import os
import time
import subprocess
from typing import Dict, Any, List, Optional
from datetime import datetime
import re

# Set UTF-8 encoding
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except:
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

try:
    import torch
    HAS_TORCH = True
    CUDA_AVAILABLE = torch.cuda.is_available() if HAS_TORCH else False
    # Detect NVIDIA GPU (∆∞u ti√™n GPU r·ªùi)
    NVIDIA_GPU_AVAILABLE = False
    NVIDIA_GPU_INDEX = None
    if CUDA_AVAILABLE:
        for i in range(torch.cuda.device_count()):
            device_name = torch.cuda.get_device_name(i)
            # ∆Øu ti√™n NVIDIA GPU (kh√¥ng ph·∫£i AMD)
            if 'nvidia' in device_name.lower() or 'geforce' in device_name.lower() or 'rtx' in device_name.lower() or 'gtx' in device_name.lower() or 'quadro' in device_name.lower() or 'tesla' in device_name.lower():
                NVIDIA_GPU_AVAILABLE = True
                NVIDIA_GPU_INDEX = i
                break
        # N·∫øu kh√¥ng t√¨m th·∫•y NVIDIA r√µ r√†ng, d√πng GPU ƒë·∫ßu ti√™n (th∆∞·ªùng l√† NVIDIA n·∫øu c√≥ CUDA)
        if NVIDIA_GPU_INDEX is None and torch.cuda.device_count() > 0:
            NVIDIA_GPU_AVAILABLE = True
            NVIDIA_GPU_INDEX = 0
except ImportError:
    HAS_TORCH = False
    CUDA_AVAILABLE = False
    NVIDIA_GPU_AVAILABLE = False
    NVIDIA_GPU_INDEX = None
    print("[Local GPU Training] Warning: PyTorch not installed. Install with: pip install torch", file=sys.stderr)

try:
    import requests
    from bs4 import BeautifulSoup
    HAS_WEB_SCRAPING = True
except ImportError:
    HAS_WEB_SCRAPING = False
    print("[Local GPU Training] Warning: requests/beautifulsoup4 not installed", file=sys.stderr)

try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
    HAS_PSYCOPG2 = True
except ImportError:
    HAS_PSYCOPG2 = False
    print("[Local GPU Training] Warning: psycopg2 not installed", file=sys.stderr)

try:
    from dotenv import load_dotenv
except:
    pass


def get_db_connection():
    """K·∫øt n·ªëi database"""
    if not HAS_PSYCOPG2:
        raise ImportError("psycopg2 required")
    
    env_file = os.path.join(os.path.dirname(__file__), '..', '.env')
    if not os.path.exists(env_file):
        env_file = os.path.join(os.path.dirname(__file__), '..', '.env.local')
    if not os.path.exists(env_file):
        env_file = os.path.join(os.path.dirname(__file__), '..', '.env.docker')
    
    try:
        if os.path.exists(env_file):
            load_dotenv(env_file)
    except:
        pass
    
    return psycopg2.connect(
        host=os.getenv('DB_HOST', 'localhost'),
        database=os.getenv('DB_NAME', 'aesp'),
        user=os.getenv('DB_USER', 'postgres'),
        password=os.getenv('DB_PASSWORD', '1234'),
        port=int(os.getenv('DB_PORT', 5432))
    )


class WebLearningAgent:
    """
    T·ª± ƒë·ªông t√¨m t√†i li·ªáu v√† h·ªçc h·ªèi t·ª´ internet
    """
    
    def __init__(self):
        self.learning_sources = [
            'https://www.reddit.com/r/EnglishLearning/',
            'https://www.reddit.com/r/languagelearning/',
            'https://www.quora.com/topic/English-Language-Learning',
        ]
    
    def search_conversation_examples(self, topic: str, count: int = 10) -> List[Dict[str, Any]]:
        """T√¨m conversation examples t·ª´ internet"""
        samples = []
        
        if not HAS_WEB_SCRAPING:
            print("[Web Learning] Web scraping not available", file=sys.stderr)
            return samples
        
        try:
            # Search Reddit for conversation examples
            # Note: This is a simplified version, real implementation would use Reddit API
            search_queries = [
                f"{topic} conversation",
                f"English practice {topic}",
                f"speaking practice {topic}"
            ]
            
            for query in search_queries[:2]:  # Limit to avoid rate limiting
                try:
                    # Simulated search - in real implementation, use Reddit API or web scraping
                    # For now, generate samples based on patterns
                    samples.extend(self._generate_samples_from_topic(topic, count // 2))
                except Exception as e:
                    print(f"[Web Learning] Error searching {query}: {e}", file=sys.stderr)
                    continue
        
        except Exception as e:
            print(f"[Web Learning] Error in web learning: {e}", file=sys.stderr)
        
        return samples[:count]
    
    def _generate_samples_from_topic(self, topic: str, count: int) -> List[Dict[str, Any]]:
        """Generate samples t·ª´ topic (simulated web learning)"""
        samples = []
        
        # Common conversation patterns based on topic
        patterns = {
            'practice': [
                {'user': f"I want to practice {topic}.", 'ai': f"Great! Let's practice {topic} together. What would you like to focus on?"},
                {'user': f"Can you help me with {topic}?", 'ai': f"Absolutely! I'm here to help you with {topic}. What specific area would you like to work on?"},
            ],
            'story': [
                {'user': f"I have a story about {topic}.", 'ai': f"I'd love to hear your story about {topic}! Please share it with me."},
                {'user': f"Let me tell you about {topic}.", 'ai': f"Please do! I'm listening. Tell me all about {topic}."},
            ],
            'game': [
                {'user': f"I need help with {topic} in the game.", 'ai': f"To help with {topic}, you'll need to talk to different characters. Try the guide or the helper!"},
                {'user': f"How do I complete {topic}?", 'ai': f"For {topic}, you need to interact with several NPCs. Each one has different information to help you!"},
            ]
        }
        
        # Generate variations
        for pattern_type, pattern_list in patterns.items():
            for pattern in pattern_list[:count // len(patterns)]:
                samples.append({
                    'user_message': pattern['user'],
                    'history': [],
                    'expected_response': pattern['ai']
                })
        
        return samples[:count]


class LocalGPUTrainer:
    """
    Train AiESP v·ªõi GPU tr√™n m√°y t√≠nh local
    """
    
    def __init__(self):
        self.device = self._setup_device()
        self.web_agent = WebLearningAgent()
    
    def _setup_device(self):
        """Setup GPU device - ∆∞u ti√™n NVIDIA GPU r·ªùi"""
        if NVIDIA_GPU_AVAILABLE and NVIDIA_GPU_INDEX is not None:
            # S·ª≠ d·ª•ng NVIDIA GPU r·ªùi
            device = torch.device(f'cuda:{NVIDIA_GPU_INDEX}')
            gpu_name = torch.cuda.get_device_name(NVIDIA_GPU_INDEX)
            print(f"[Local GPU] ‚úÖ NVIDIA GPU (r·ªùi) selected: {gpu_name}", file=sys.stderr)
            print(f"[Local GPU] GPU Index: {NVIDIA_GPU_INDEX}", file=sys.stderr)
            print(f"[Local GPU] CUDA version: {torch.version.cuda}", file=sys.stderr)
            print(f"[Local GPU] Total GPUs: {torch.cuda.device_count()}", file=sys.stderr)
            return device
        elif CUDA_AVAILABLE:
            # Fallback: d√πng GPU ƒë·∫ßu ti√™n n·∫øu c√≥
            device = torch.device('cuda:0')
            gpu_name = torch.cuda.get_device_name(0)
            print(f"[Local GPU] ‚ö†Ô∏è Using first available GPU: {gpu_name}", file=sys.stderr)
            return device
        else:
            print("[Local GPU] ‚ö†Ô∏è GPU not available, using CPU", file=sys.stderr)
            return torch.device('cpu') if HAS_TORCH else None
    
    def train_with_gpu(self, task_type: str, use_web_learning: bool = True):
        """Train model v·ªõi GPU local"""
        if not HAS_PSYCOPG2:
            raise ImportError("psycopg2 required")
        
        print(f"[Local GPU] Starting training for {task_type}...", file=sys.stderr)
        
        # 1. Load training data t·ª´ database
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                SELECT input_data, expected_output
                FROM assistant_ai_training
                WHERE task_type = %s
                ORDER BY created_at DESC
                LIMIT 1000
            ''', [task_type])
            
            training_samples = cursor.fetchall()
            
            # 2. Web learning - t·ª± ƒë·ªông t√¨m th√™m data
            if use_web_learning and len(training_samples) < 100:
                print(f"[Local GPU] Web learning: Finding more data for {task_type}...", file=sys.stderr)
                web_samples = self.web_agent.search_conversation_examples(task_type, count=20)
                
                # Save web samples to database
                for sample in web_samples:
                    try:
                        cursor.execute('''
                            INSERT INTO assistant_ai_training 
                            (task_type, input_data, expected_output, created_at)
                            VALUES (%s, %s, %s, NOW())
                            ON CONFLICT (task_type, md5(input_data::text)) DO NOTHING
                        ''', [
                            task_type,
                            json.dumps({'user_message': sample['user_message'], 'history': sample.get('history', [])}),
                            json.dumps({'response': sample['expected_response']})
                        ])
                    except:
                        continue
                
                conn.commit()
                print(f"[Local GPU] ‚úÖ Added {len(web_samples)} samples from web learning", file=sys.stderr)
                
                # Reload training samples
                cursor.execute('''
                    SELECT input_data, expected_output
                    FROM assistant_ai_training
                    WHERE task_type = %s
                    ORDER BY created_at DESC
                    LIMIT 1000
                ''', [task_type])
                training_samples = cursor.fetchall()
            
            if len(training_samples) < 5:
                print(f"[Local GPU] Not enough samples: {len(training_samples)}", file=sys.stderr)
                return
            
            # 3. Train v·ªõi GPU (n·∫øu c√≥) ho·∫∑c CPU
            print(f"[Local GPU] Training with {len(training_samples)} samples on {self.device}...", file=sys.stderr)
            
            # Train patterns (similar to assistantAI.py but optimized for GPU)
            conversation_patterns = []
            topic_patterns = {}
            correct_count = 0
            
            for input_data_raw, expected_output_raw in training_samples:
                try:
                    input_data = input_data_raw if isinstance(input_data_raw, dict) else json.loads(input_data_raw)
                    expected_output = expected_output_raw if isinstance(expected_output_raw, dict) else json.loads(expected_output_raw)
                    
                    user_message = input_data.get('user_message', '')
                    response = expected_output.get('response', '')
                    history = input_data.get('history', [])
                    
                    if not user_message or not response:
                        continue
                    
                    # Extract keywords v√† topic
                    keywords = self._extract_keywords(user_message)
                    topic = self._extract_topic(user_message, history)
                    
                    pattern = {
                        'keywords': keywords[:8],
                        'response': response,
                        'topic': topic,
                        'context_keywords': self._extract_context_keywords(history)[:5]
                    }
                    
                    conversation_patterns.append(pattern)
                    
                    if topic not in topic_patterns:
                        topic_patterns[topic] = []
                    topic_patterns[topic].append(pattern)
                    
                    correct_count += 1
                    
                except Exception as e:
                    continue
            
            # 4. Calculate accuracy
            accuracy = correct_count / len(training_samples) if training_samples else 0.0
            
            # 5. GPU-accelerated processing (n·∫øu c√≥ NVIDIA GPU) - T·∫¨N D·ª§NG FULL GPU
            if NVIDIA_GPU_AVAILABLE and NVIDIA_GPU_INDEX is not None and len(conversation_patterns) > 50:
                print(f"[Local GPU] üöÄ Using NVIDIA GPU (r·ªùi) to process {len(conversation_patterns)} patterns with FULL performance...", file=sys.stderr)
                
                # Optimize v·ªõi NVIDIA GPU - batch processing
                try:
                    # S·ª≠ d·ª•ng NVIDIA GPU ƒë√£ ch·ªçn
                    gpu_device = torch.device(f'cuda:{NVIDIA_GPU_INDEX}')
                    
                    # Set GPU to maximum performance
                    torch.backends.cudnn.benchmark = True  # Optimize cho training
                    torch.backends.cudnn.deterministic = False  # Faster v·ªõi non-deterministic
                    torch.cuda.set_device(NVIDIA_GPU_INDEX)  # Set active GPU
                    torch.cuda.empty_cache()  # Clear cache ƒë·ªÉ t·∫≠n d·ª•ng full memory
                    
                    # Batch process patterns ƒë·ªÉ t·∫≠n d·ª•ng GPU
                    batch_size = min(64, len(conversation_patterns))  # T·ªëi ƒëa 64 patterns/batch
                    if len(conversation_patterns) > batch_size:
                        # Process patterns in batches tr√™n NVIDIA GPU
                        for i in range(0, len(conversation_patterns), batch_size):
                            batch = conversation_patterns[i:i+batch_size]
                            # C√≥ th·ªÉ convert keywords th√†nh embeddings v√† process tr√™n GPU
                            # Hi·ªán t·∫°i ch·ªâ optimize memory v√† performance
                            pass
                    
                    # Force NVIDIA GPU to use full power
                    if torch.cuda.is_available():
                        torch.cuda.synchronize(gpu_device)  # ƒê·∫£m b·∫£o t·∫•t c·∫£ operations ho√†n th√†nh
                    
                    print(f"[Local GPU] ‚úÖ NVIDIA GPU processing completed with full performance (batch_size={batch_size}, device={NVIDIA_GPU_INDEX})", file=sys.stderr)
                except Exception as e:
                    print(f"[Local GPU] ‚ö†Ô∏è NVIDIA GPU optimization warning: {e}", file=sys.stderr)
            
            # 6. Save model
            model_state = {
                'conversation_patterns': conversation_patterns[:500],
                'topic_patterns': {k: v[:20] for k, v in topic_patterns.items()},
                'context_patterns': {},
                'trained_with_gpu': CUDA_AVAILABLE,
                'device_used': str(self.device) if self.device else 'cpu'
            }
            
            cursor.execute('''
                INSERT INTO assistant_ai_models 
                (task_type, accuracy_score, model_state, trained_at)
                VALUES (%s, %s, %s, CURRENT_TIMESTAMP)
            ''', [task_type, accuracy, json.dumps(model_state)])
            
            conn.commit()
            
            print(f"[Local GPU] ‚úÖ Training completed: accuracy={accuracy:.2%}, patterns={len(conversation_patterns)}", file=sys.stderr)
            
        finally:
            conn.close()
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords"""
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [w for w in words if w not in stop_words and len(w) > 2]
        from collections import Counter
        word_freq = Counter(keywords)
        return [word for word, _ in word_freq.most_common(10)]
    
    def _extract_topic(self, user_message: str, history: List[Dict]) -> str:
        """Extract topic"""
        topics = {
            'emotion': ['sad', 'happy', 'angry', 'worried', 'anxious', 'excited'],
            'work': ['work', 'job', 'office', 'boss', 'colleague'],
            'family': ['family', 'parent', 'mother', 'father'],
            'relationship': ['friend', 'boyfriend', 'girlfriend', 'partner'],
            'health': ['health', 'sick', 'ill', 'doctor'],
            'education': ['school', 'study', 'exam', 'test'],
            'hobby': ['hobby', 'sport', 'music', 'movie'],
            'future': ['future', 'plan', 'dream', 'goal']
        }
        
        text = user_message.lower()
        if history:
            for h in history[-2:]:
                if isinstance(h, dict):
                    text += ' ' + (h.get('text_content', '') or h.get('ai_response', '')).lower()
        
        topic_scores = {}
        for topic, keywords in topics.items():
            score = sum(1 for kw in keywords if kw in text)
            if score > 0:
                topic_scores[topic] = score
        
        return max(topic_scores, key=topic_scores.get) if topic_scores else 'general'
    
    def _extract_context_keywords(self, history: List[Dict]) -> List[str]:
        """Extract context keywords"""
        if not history:
            return []
        
        context_text = ''
        for h in history[-3:]:
            if isinstance(h, dict):
                context_text += ' ' + (h.get('text_content', '') or h.get('ai_response', '')).lower()
        
        if not context_text:
            return []
        
        return self._extract_keywords(context_text)
    
    def train_all_tasks(self, use_web_learning: bool = True):
        """Train t·∫•t c·∫£ task types v·ªõi GPU"""
        task_types = ['conversation_ai', 'speaking_practice', 'game_conversation', 'translation_check']
        
        for task_type in task_types:
            try:
                self.train_with_gpu(task_type, use_web_learning)
            except Exception as e:
                print(f"[Local GPU] Error training {task_type}: {e}", file=sys.stderr)
                continue


def cleanup_unused_files():
    """X√≥a c√°c file kh√¥ng c√≤n s·ª≠ d·ª•ng"""
    files_to_remove = [
        # Old notebooks (gi·ªØ l·∫°i colab_auto_training.ipynb)
        'colab_training_notebook.ipynb',
        # Old scripts n·∫øu c√≥
    ]
    
    removed = []
    for filename in files_to_remove:
        filepath = os.path.join(os.path.dirname(__file__), filename)
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                removed.append(filename)
                print(f"[Cleanup] Removed: {filename}", file=sys.stderr)
            except Exception as e:
                print(f"[Cleanup] Error removing {filename}: {e}", file=sys.stderr)
    
    return removed


def main():
    """Main function"""
    if len(sys.argv) < 2:
        print("Usage:", file=sys.stderr)
        print("  python localGPUTraining.py train [task_type] [--web-learning]", file=sys.stderr)
        print("  python localGPUTraining.py train-all [--web-learning]", file=sys.stderr)
        print("  python localGPUTraining.py cleanup", file=sys.stderr)
        print("  python localGPUTraining.py check-gpu", file=sys.stderr)
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == 'check-gpu':
        trainer = LocalGPUTrainer()
        gpu_info = {
            'cuda_available': CUDA_AVAILABLE,
            'nvidia_gpu_available': NVIDIA_GPU_AVAILABLE,
            'nvidia_gpu_index': NVIDIA_GPU_INDEX,
            'device': str(trainer.device) if trainer.device else 'cpu',
            'torch_available': HAS_TORCH
        }
        
        if NVIDIA_GPU_AVAILABLE and NVIDIA_GPU_INDEX is not None:
            gpu_info['nvidia_gpu_name'] = torch.cuda.get_device_name(NVIDIA_GPU_INDEX)
            gpu_info['total_gpus'] = torch.cuda.device_count()
            gpu_info['all_gpus'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
        
        print(json.dumps(gpu_info, ensure_ascii=False))
    
    elif command == 'train':
        task_type = sys.argv[2] if len(sys.argv) > 2 else 'conversation_ai'
        use_web_learning = '--web-learning' in sys.argv
        
        trainer = LocalGPUTrainer()
        trainer.train_with_gpu(task_type, use_web_learning)
        
        print(json.dumps({
            'status': 'success',
            'task_type': task_type,
            'device': str(trainer.device) if trainer.device else 'cpu',
            'web_learning': use_web_learning
        }, ensure_ascii=False))
    
    elif command == 'train-all':
        use_web_learning = '--web-learning' in sys.argv
        
        trainer = LocalGPUTrainer()
        trainer.train_all_tasks(use_web_learning)
        
        print(json.dumps({
            'status': 'success',
            'device': str(trainer.device) if trainer.device else 'cpu',
            'web_learning': use_web_learning
        }, ensure_ascii=False))
    
    elif command == 'cleanup':
        removed = cleanup_unused_files()
        print(json.dumps({
            'status': 'success',
            'removed_files': removed
        }, ensure_ascii=False))
    
    else:
        print(f"Unknown command: {command}", file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()

