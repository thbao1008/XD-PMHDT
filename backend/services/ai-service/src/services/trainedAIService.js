/**
 * Trained AI Service - Wrapper ƒë·ªÉ g·ªçi OpenRouter qua ai_models training
 * T·∫•t c·∫£ AI calls s·∫Ω ƒëi qua training layer ƒë·ªÉ c√≥ c∆∞·ªùng ƒë·ªô t∆∞ duy cao
 */

import * as aiService from "./aiService.js";
import { spawn } from "child_process";
import path from "path";
import fs from "fs";
import { fileURLToPath } from "url";
import { findPythonExecutable } from "../utils/whisperxRunner.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Get backend directory (go up from ai-service/src/services to backend)
// __dirname = backend/services/ai-service/src/services
// Go up 4 levels: services -> ai-service -> services -> backend
const backendDir = path.resolve(__dirname, "..", "..", "..", "..");

/**
 * G·ªçi Python trainer ƒë·ªÉ t·∫°o training data tr∆∞·ªõc khi g·ªçi OpenRouter
 * S·ª≠ d·ª•ng stdin ƒë·ªÉ tr√°nh l·ªói k√Ω t·ª± ƒë·∫∑c bi·ªát tr√™n Windows
 */
async function getTrainingDataFromPython(trainingType, options = {}) {
  return new Promise((resolve, reject) => {
    try {
      const trainerPath = path.resolve(backendDir, "ai_models", "comprehensiveAITrainer.py");
      
      // Ki·ªÉm tra xem file c√≥ t·ªìn t·∫°i kh√¥ng
      if (!fs.existsSync(trainerPath)) {
        console.warn(`‚ö†Ô∏è comprehensiveAITrainer.py not found at ${trainerPath}, using fallback`);
        // Resolve v·ªõi null ƒë·ªÉ trigger fallback
        return resolve(null);
      }
      
      // T·∫°o data object ƒë·ªÉ g·ª≠i qua stdin
      let stdinData = { training_type: trainingType };
      
      if (trainingType === 'prompt_generator') {
        stdinData = {
          training_type: 'prompt_generator',
          level: options.level || 2,
          used_topics: options.usedTopics || [],
          used_prompts: options.usedPrompts || [],
          topics_json: options.topicsJson || "[]",
          challenges_json: options.challengesJson || "[]",
          learner_id: options.learnerId || null,
          personalization_context: options.personalizationContext || null
        };
      } else if (trainingType === 'conversation_ai') {
        stdinData = {
          training_type: 'conversation_ai',
          topic: options.topic || null,
          history: options.history || []
        };
      } else if (trainingType === 'quick_analysis') {
        stdinData = {
          training_type: 'quick_analysis',
          transcript: options.transcript || "",
          expected: options.expected || null,
          level: options.level || 2
        };
      }
      
      // T·ª± ƒë·ªông t√¨m Python executable (gi·ªëng nh∆∞ whisperxRunner)
      const pythonCmd = findPythonExecutable();
      console.log(`[trainedAIService] Using Python command: ${pythonCmd}`);
      
      // N·∫øu pythonCmd c√≥ flag (nh∆∞ "py -3"), split ra
      const [pythonExec, ...pythonFlags] = pythonCmd.split(' ');
      const finalArgs = [...pythonFlags, trainerPath];
      
      // Spawn Python process v·ªõi stdin v√† set UTF-8 encoding
      const pythonProcess = spawn(pythonExec, finalArgs, {
        stdio: ['pipe', 'pipe', 'pipe'],
        shell: process.platform === 'win32', // Windows c·∫ßn shell ƒë·ªÉ t√¨m py launcher
        env: {
          ...process.env,
          PYTHONIOENCODING: 'utf-8',
          PYTHONUTF8: '1'
        }
      });
      
      let stdout = '';
      let stderr = '';
      
      pythonProcess.stdout.on('data', (data) => {
        stdout += data.toString();
      });
      
      pythonProcess.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      pythonProcess.on('close', (code) => {
        if (code !== 0) {
          console.error("‚ùå Python trainer error:", stderr);
          reject(new Error(`Python trainer exited with code ${code}: ${stderr}`));
          return;
        }
        
        try {
          // Extract JSON from stdout (Python may output debug messages before JSON)
          // Find the first '{' and last '}' to extract the JSON object
          const firstBrace = stdout.indexOf('{');
          const lastBrace = stdout.lastIndexOf('}');
          
          if (firstBrace === -1 || lastBrace === -1 || firstBrace >= lastBrace) {
            throw new Error("No valid JSON found in Python output");
          }
          
          const jsonString = stdout.substring(firstBrace, lastBrace + 1);
          const result = JSON.parse(jsonString);
          resolve(result);
        } catch (err) {
          console.error("‚ùå Error parsing Python output:", err);
          console.error("Python stdout:", stdout);
          reject(new Error(`Failed to parse Python output: ${err.message}`));
        }
      });
      
      pythonProcess.on('error', (err) => {
        console.error("‚ùå Error spawning Python process:", err);
        reject(err);
      });
      
      // G·ª≠i data qua stdin
      pythonProcess.stdin.write(JSON.stringify(stdinData));
      pythonProcess.stdin.end();
      
    } catch (err) {
      console.error("‚ùå Error calling Python trainer:", err);
      reject(err);
    }
  });
}

/**
 * T·∫°o random seed v√† unique identifier cho m·ªói request
 */
function generateRandomSeed() {
  return Math.floor(Math.random() * 1000000) + Date.now();
}

/**
 * T·∫°o unique user message v·ªõi randomization
 */
function createRandomizedUserMessage(trainingType, seed) {
  const timestamp = Date.now();
  const randomVariations = [
    `Generate a COMPLETELY NEW and UNIQUE topic and prompt. Use seed ${seed} and timestamp ${timestamp}. Return JSON: {"topic": "topic name", "description": "brief", "suggested_prompt": "sentence"}`,
    `Create a FRESH topic that is DIFFERENT from all previous ones. Random seed: ${seed}. Return JSON: {"topic": "topic name", "description": "brief", "suggested_prompt": "sentence"}`,
    `Generate a NEW topic with maximum creativity. Seed: ${seed}, Time: ${timestamp}. Return JSON: {"topic": "topic name", "description": "brief", "suggested_prompt": "sentence"}`,
    `Create a UNIQUE topic that hasn't been used before. Random: ${seed}. Return JSON: {"topic": "topic name", "description": "brief", "suggested_prompt": "sentence"}`
  ];
  
  // Ch·ªçn random variation d·ª±a tr√™n seed
  const variationIndex = seed % randomVariations.length;
  return randomVariations[variationIndex];
}

/**
 * G·ªçi OpenRouter v·ªõi training data t·ª´ Python trainer
 * ƒê√¢y l√† h√†m ch√≠nh ƒë·ªÉ thay th·∫ø callOpenRouter tr·ª±c ti·∫øp
 * V·ªõi randomization ƒë·ªÉ ƒë·∫£m b·∫£o ƒëa d·∫°ng
 */
export async function callTrainedAI(trainingType, options = {}, messages = null, aiOpts = {}) {
  try {
    // T·∫°o random seed cho m·ªói request ƒë·ªÉ ƒë·∫£m b·∫£o ƒëa d·∫°ng
    const randomSeed = generateRandomSeed();
    const timestamp = Date.now();
    
    // L·∫•y training data t·ª´ Python trainer
    const trainingData = await getTrainingDataFromPython(trainingType, options);
    
    // N·∫øu training fail, fallback v·ªÅ callOpenRouter tr·ª±c ti·∫øp
    if (!trainingData || !trainingData.system_prompt) {
      console.warn("‚ö†Ô∏è Training data not available, using direct OpenRouter call");
      if (messages) {
        return await aiService.callOpenRouter(messages, aiOpts);
      }
      throw new Error("No training data and no messages provided");
    }
    
    // Th√™m randomization v√†o system prompt
    const randomizedSystemPrompt = `${trainingData.system_prompt}

RANDOMIZATION PARAMETERS (Critical for diversity):
- Random Seed: ${randomSeed}
- Timestamp: ${timestamp}
- Request ID: ${Math.random().toString(36).substring(7)}
- Use stochastic sampling with high creativity
- Ensure this topic is COMPLETELY DIFFERENT from any previous topics
- Vary sentence structure, vocabulary, and topic angle`;

    // T·∫°o messages t·ª´ training data
    const trainedMessages = [
      { role: 'system', content: randomizedSystemPrompt }
    ];
    
    // Th√™m user messages n·∫øu c√≥
    if (messages && Array.isArray(messages)) {
      // T√¨m user messages trong messages array
      const userMessages = messages.filter(m => m.role === 'user');
      trainedMessages.push(...userMessages);
    } else if (typeof messages === 'string') {
      trainedMessages.push({ role: 'user', content: messages });
    } else if (trainingType === 'prompt_generator') {
      // T·∫°o randomized user message
      const randomizedUserMessage = createRandomizedUserMessage(trainingType, randomSeed);
      trainedMessages.push({ 
        role: 'user', 
        content: randomizedUserMessage
      });
    } else if (trainingType === 'quick_analysis') {
      trainedMessages.push({ 
        role: 'user', 
        content: `Analyze now. Seed: ${randomSeed}. Return JSON only.` 
      });
    }
    
    // T√≠nh to√°n sampling parameters ƒë·ªÉ tƒÉng ƒëa d·∫°ng
    const baseTemperature = aiOpts.temperature || (trainingData.config?.temperature || 0.95);
    // TƒÉng temperature th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o ƒëa d·∫°ng
    const enhancedTemperature = Math.min(1.2, baseTemperature + 0.1);
    
    // G·ªçi OpenRouter v·ªõi trained messages v√† enhanced sampling
    let maxTokens = aiOpts.max_tokens || (trainingData.config?.max_tokens || (trainingType === 'quick_analysis' ? 500 : 250));
    
    try {
      const response = await aiService.callOpenRouter(
        trainedMessages,
        {
          model: aiOpts.model || 'openai/gpt-4o-mini',
          temperature: enhancedTemperature,
          max_tokens: maxTokens,
          top_p: 0.95, // Nucleus sampling ƒë·ªÉ tƒÉng ƒëa d·∫°ng
          frequency_penalty: 0.5, // Penalty cho repetition
          presence_penalty: 0.5 // Penalty cho presence c·ªßa t·ª´ ƒë√£ d√πng
        }
      );
      
      // Log ƒë·ªÉ debug
      console.log(`üé≤ Generated topic with seed: ${randomSeed}, temperature: ${enhancedTemperature}`);
      
      return response;
    } catch (err) {
      // X·ª≠ l√Ω l·ªói payment required (402) - t·ª± ƒë·ªông gi·∫£m max_tokens v√† retry
      if (err.status === 402 && err.code === 'PAYMENT_REQUIRED' && err.maxAffordableTokens) {
        console.warn(`‚ö†Ô∏è Payment required. Retrying with reduced max_tokens: ${err.maxAffordableTokens}`);
        
        // Retry v·ªõi max_tokens gi·∫£m xu·ªëng (tr·ª´ 10 ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n)
        const reducedTokens = Math.max(50, err.maxAffordableTokens - 10);
        
        try {
          const retryResponse = await aiService.callOpenRouter(
            trainedMessages,
            {
              model: aiOpts.model || 'openai/gpt-4o-mini',
              temperature: enhancedTemperature,
              max_tokens: reducedTokens,
              top_p: 0.95,
              frequency_penalty: 0.5,
              presence_penalty: 0.5
            }
          );
          
          console.log(`‚úÖ Retry successful with max_tokens: ${reducedTokens}`);
          return retryResponse;
        } catch (retryErr) {
          console.error("‚ùå Retry failed:", retryErr);
          // N·∫øu retry v·∫´n fail, fallback v·ªÅ direct call ho·∫∑c throw error
          if (messages) {
            try {
              return await aiService.callOpenRouter(messages, { ...aiOpts, max_tokens: reducedTokens });
            } catch (fallbackErr) {
              console.error("‚ùå Fallback also failed:", fallbackErr);
              throw err; // Throw original error
            }
          }
          throw err;
        }
      }
      
      // X·ª≠ l√Ω c√°c l·ªói kh√°c
      console.error("‚ùå Error in callTrainedAI:", err);
      // Fallback v·ªÅ direct call n·∫øu c√≥ messages
      if (messages) {
        try {
          return await aiService.callOpenRouter(messages, aiOpts);
        } catch (fallbackErr) {
          throw err; // Throw original error n·∫øu fallback c≈©ng fail
        }
      }
      throw err;
    }
  } catch (err) {
    console.error("‚ùå Error in callTrainedAI (outer catch):", err);
    throw err;
  }
}

/**
 * Wrapper cho prompt generation v·ªõi training
 */
export async function generatePromptWithTraining(level, usedTopics = [], usedPrompts = [], 
                                                 learnerId = null, sessionId = null,
                                                 topicsJson = "[]", challengesJson = "[]",
                                                 personalizationContext = null) {
  return await callTrainedAI('prompt_generator', {
    level,
    usedTopics,
    usedPrompts,
    learnerId,
    sessionId,
    topicsJson,
    challengesJson,
    personalizationContext
  }, null, {
    model: 'openai/gpt-4o-mini',
    temperature: 0.95,
    max_tokens: 250
  });
}

/**
 * Wrapper cho conversation AI v·ªõi training
 */
export async function conversationAIWithTraining(topic = null, history = [], userMessage = null) {
  const messages = userMessage ? [{ role: 'user', content: userMessage }] : null;
  
  return await callTrainedAI('conversation_ai', {
    topic,
    history
  }, messages, {
    model: 'openai/gpt-4o-mini',
    temperature: 0.9,
    max_tokens: 300
  });
}

/**
 * Wrapper cho quick analysis v·ªõi training
 */
export async function quickAnalysisWithTraining(transcript, expectedText = null, level = 2) {
  return await callTrainedAI('quick_analysis', {
    transcript,
    expected: expectedText,
    level
  }, null, {
    model: 'openai/gpt-4o-mini',
    temperature: 0.5,
    max_tokens: 200
  });
}

/**
 * Export callOpenRouter ƒë·ªÉ backward compatibility
 * Nh∆∞ng khuy·∫øn kh√≠ch s·ª≠ d·ª•ng callTrainedAI
 */
export { callOpenRouter } from "./aiService.js";

