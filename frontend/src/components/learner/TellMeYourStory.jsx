import React, { useState, useRef, useEffect } from "react";
import { getAuth } from "../../utils/auth";
import api from "../../api";
import { FaMicrophone, FaRedo } from "react-icons/fa";
import "../../styles/tell-me-story.css";

// Component tooltip th√¥ng minh v·ªõi t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh v·ªã tr√≠
const WordTooltipWrapper = ({ word, cleanWord, isOpen, tooltip, loading, onWordClick, onClose, micButtonRef }) => {
  const wordRef = useRef(null);
  const tooltipRef = useRef(null);
  const [tooltipPosition, setTooltipPosition] = useState('top'); // 'top' or 'bottom'

  useEffect(() => {
    if (isOpen && tooltipRef.current) {
      // ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ tooltip render xong
      setTimeout(() => {
        if (tooltipRef.current) {
          const tooltip = tooltipRef.current;
          const tooltipHeight = tooltip.offsetHeight || 180;
          const tooltipWidth = tooltip.offsetWidth || 280;
          const viewportWidth = window.innerWidth;
          const viewportHeight = window.innerHeight;
          const padding = 20;
          
          // T√¨m mic button ƒë·ªÉ ƒë·∫∑t tooltip g·∫ßn ƒë√≥
          const micButton = (micButtonRef && micButtonRef.current) || 
                           document.querySelector('.story-input button') ||
                           document.querySelector('button[style*="borderRadius"]');
          
          if (micButton) {
            const micRect = micButton.getBoundingClientRect();
            // ƒê·∫∑t tooltip b√™n ph·∫£i mic button, cƒÉn gi·ªØa theo chi·ªÅu d·ªçc
            let left = micRect.right + 20;
            let top = micRect.top + (micRect.height / 2) - (tooltipHeight / 2);
            
            // N·∫øu kh√¥ng ƒë·ªß kh√¥ng gian b√™n ph·∫£i, ƒë·∫∑t b√™n tr√°i
            if (left + tooltipWidth + padding > viewportWidth) {
              left = micRect.left - tooltipWidth - 20;
              if (left < padding) {
                left = padding;
              }
            }
            
            // ƒê·∫£m b·∫£o kh√¥ng b·ªã tr√†n ph√≠a tr√™n/d∆∞·ªõi
            if (top + tooltipHeight > viewportHeight - padding) {
              top = viewportHeight - tooltipHeight - padding;
            }
            if (top < padding) {
              top = padding;
            }
            
            tooltip.style.top = `${top}px`;
            tooltip.style.left = `${left}px`;
          } else {
            // Fallback: ƒë·∫∑t ·ªü g√≥c d∆∞·ªõi b√™n ph·∫£i
            tooltip.style.bottom = `${padding}px`;
            tooltip.style.right = `${padding}px`;
            tooltip.style.top = 'auto';
            tooltip.style.left = 'auto';
          }
        }
      }, 50);
    }
  }, [isOpen, tooltip]);

  return (
    <span
      ref={wordRef}
      onClick={onWordClick}
      style={{
        cursor: "pointer",
        position: "relative",
        display: "inline-block",
        padding: "2px 4px",
        borderRadius: "3px",
        transition: "background-color 0.2s",
        backgroundColor: isOpen ? "#e0f2fe" : "transparent"
      }}
      onMouseEnter={(e) => {
        if (!isOpen) {
          e.target.style.backgroundColor = "#f0f9ff";
        }
      }}
      onMouseLeave={(e) => {
        if (!isOpen) {
          e.target.style.backgroundColor = "transparent";
        }
      }}
    >
      {word}
      {isOpen && tooltip && (
        <div
          ref={tooltipRef}
          style={{
            position: "fixed",
            zIndex: 10000,
            background: "white",
            border: "2px solid #10b981",
            borderRadius: 12,
            padding: 16,
            boxShadow: "0 8px 32px rgba(0, 0, 0, 0.2)",
            minWidth: 240,
            maxWidth: 340,
            fontSize: 14,
            pointerEvents: "auto",
            animation: "fadeIn 0.2s ease-in"
          }}
          onClick={(e) => e.stopPropagation()}
        >
          <div style={{ marginBottom: 8 }}>
            <strong style={{ fontSize: 16, color: "#10b981" }}>{tooltip.word}</strong>
            {loading && (
              <span style={{ marginLeft: 8, color: "#999", fontSize: 12 }}>ƒêang t·∫£i...</span>
            )}
          </div>
          {tooltip.pronunciation && (
            <div style={{ marginBottom: 6, color: "#10b981", fontWeight: "bold" }}>
              <strong>Ph√°t √¢m:</strong> /{tooltip.pronunciation}/
            </div>
          )}
          {tooltip.definition && (
            <div style={{ marginBottom: 6 }}>
              <strong>Nghƒ©a:</strong> {tooltip.definition}
            </div>
          )}
          <button
            onClick={onClose}
            style={{
              marginTop: 8,
              padding: "4px 12px",
              background: "#6b7280",
              color: "white",
              border: "none",
              borderRadius: 4,
              cursor: "pointer",
              fontSize: 12
            }}
          >
            ƒê√≥ng
          </button>
        </div>
      )}
    </span>
  );
};

export default function TellMeYourStory({ onBack }) {
  const auth = getAuth();
  const [conversation, setConversation] = useState([]);
  const [isRecording, setIsRecording] = useState(false);
  const [sending, setSending] = useState(false);
  const [sessionId, setSessionId] = useState(null);
  const [playingAudio, setPlayingAudio] = useState(null); // Track which message is playing TTS
  const [wordTooltip, setWordTooltip] = useState(null); // Dictionary tooltip
  const [openWordTooltip, setOpenWordTooltip] = useState(null); // Word ƒëang m·ªü tooltip
  const [loadingWord, setLoadingWord] = useState(false); // Loading state cho dictionary
  const [hasRecordedAudio, setHasRecordedAudio] = useState(false); // Track if there's recorded audio to send
  const [voiceType, setVoiceType] = useState('female'); // 'male' ho·∫∑c 'female'
  const [voiceOrigin, setVoiceOrigin] = useState('native'); // 'native' ho·∫∑c 'asian'
  const [showVoiceSubmenu, setShowVoiceSubmenu] = useState(null); // 'male', 'female', ho·∫∑c null
  const [aiAudioUrls, setAiAudioUrls] = useState({}); // L∆∞u audio URLs cho AI messages
  const userAudioRefs = useRef({}); // L∆∞u audio refs cho user messages
  const messagesEndRef = useRef(null);
  const micButtonRef = useRef(null); // Ref cho mic button
  
  // Audio recording refs
  const mediaRecorderRef = useRef(null);
  const mediaStreamRef = useRef(null);
  const audioChunksRef = useRef([]);
  const recognitionRef = useRef(null); // Web Speech API for speech end detection
  const silenceTimerRef = useRef(null); // Timer ƒë·ªÉ ph√°t hi·ªán im l·∫∑ng
  const lastSoundTimeRef = useRef(null); // Th·ªùi gian c√≥ √¢m thanh cu·ªëi c√πng
  
  // Timeout 30 ph√∫t cho conversation history
  const conversationTimeoutRef = useRef(null);
  const lastActivityRef = useRef(Date.now());

  useEffect(() => {
    startConversation();
    // Kh√¥ng x√≥a conversation khi unmount - ch·ªâ cleanup audio/recording
    return () => {
      // Cleanup: stop recording and close streams
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
      }
      window.speechSynthesis.cancel();
      // Clear timeout n·∫øu c√≥
      if (conversationTimeoutRef.current) {
        clearTimeout(conversationTimeoutRef.current);
      }
    };
  }, []);
  
  // Reset timeout m·ªói khi c√≥ message m·ªõi
  useEffect(() => {
    if (conversation.length > 0) {
      lastActivityRef.current = Date.now();
      
      // Clear timeout c≈©
      if (conversationTimeoutRef.current) {
        clearTimeout(conversationTimeoutRef.current);
      }
      
      // Set timeout m·ªõi: 30 ph√∫t = 30 * 60 * 1000 = 1800000ms
      conversationTimeoutRef.current = setTimeout(() => {
        // N·∫øu kh√¥ng c√≥ ho·∫°t ƒë·ªông trong 30 ph√∫t, x√≥a conversation v√† t·∫°o m·ªõi
        const timeSinceLastActivity = Date.now() - lastActivityRef.current;
        if (timeSinceLastActivity >= 30 * 60 * 1000) {
          handleNewConversation();
        }
      }, 30 * 60 * 1000); // 30 ph√∫t
    }
    
    return () => {
      if (conversationTimeoutRef.current) {
        clearTimeout(conversationTimeoutRef.current);
      }
    };
  }, [conversation]);

  // Cleanup audio URLs when component unmounts
  useEffect(() => {
    return () => {
      Object.values(aiAudioUrls).forEach(url => {
        if (url && url.startsWith('blob:')) {
          URL.revokeObjectURL(url);
        }
      });
    };
  }, [aiAudioUrls]);

  useEffect(() => {
    scrollToBottom();
  }, [conversation]);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  const startConversation = async () => {
    try {
      const userId = auth?.user?._id || auth?.user?.id;
      const learnerId = auth?.user?.learner_id;
      
      if (!userId && !learnerId) return;

      const res = await api.post("/learners/speaking-practice/story/sessions", {
        learner_id: learnerId,
        user_id: userId
      });

      setSessionId(res.data.session_id);
      const initialMessage = res.data.initial_message ||
        "Hello! I'm your friend. Please tell me your story by speaking into the microphone!";
      
      setConversation([
        {
          type: "ai",
          text: initialMessage,
          timestamp: new Date(),
          id: 'initial'
        }
      ]);
      
      // Reset last activity
      lastActivityRef.current = Date.now();
      
      // T·ª± ƒë·ªông ph√°t √¢m message ƒë·∫ßu ti√™n
      speakText(initialMessage, 'initial');
    } catch (err) {
      console.error("‚ùå Error starting conversation:", err);
    }
  };
  
  // H√†m t·∫°o cu·ªôc tr√≤ chuy·ªán m·ªõi
  const handleNewConversation = async () => {
    // Clear timeout
    if (conversationTimeoutRef.current) {
      clearTimeout(conversationTimeoutRef.current);
    }
    
    // X√≥a conversation hi·ªán t·∫°i
    setConversation([]);
    setSessionId(null);
    
    // Cleanup audio URLs
    Object.values(aiAudioUrls).forEach(url => {
      if (url && url.startsWith('blob:')) {
        URL.revokeObjectURL(url);
      }
    });
    setAiAudioUrls({});
    userAudioRefs.current = {};
    
    // D·ª´ng TTS n·∫øu ƒëang ph√°t
    window.speechSynthesis.cancel();
    setPlayingAudio(null);
    
    // T·∫°o conversation m·ªõi
    await startConversation();
  };

  // Text-to-Speech cho AI response v·ªõi gi·ªçng ƒë√£ ch·ªçn
  // S·ª≠ d·ª•ng CSM (Sesame AI) cho gi·ªçng native, FPT.AI cho gi·ªçng asian, fallback v·ªÅ browser TTS
  const speakText = async (text, messageId, overrideVoiceType = null, overrideVoiceOrigin = null) => {
    // D·ª´ng b·∫•t k·ª≥ ph√°t √¢m n√†o ƒëang ch·∫°y
    window.speechSynthesis.cancel();
    
    // L·∫•y gi√° tr·ªã hi·ªán t·∫°i c·ªßa voiceType v√† voiceOrigin (ƒë·∫£m b·∫£o d√πng ƒë√∫ng gi·ªçng ƒë√£ ch·ªçn)
    // Cho ph√©p override ƒë·ªÉ ƒë·∫£m b·∫£o d√πng ƒë√∫ng gi·ªçng khi thay ƒë·ªïi
    const currentVoiceType = overrideVoiceType !== null ? overrideVoiceType : voiceType;
    const currentVoiceOrigin = overrideVoiceOrigin !== null ? overrideVoiceOrigin : voiceOrigin;
    
    // X√¢y d·ª±ng context t·ª´ conversation history cho CSM (ch·ªâ l·∫•y c√°c message g·∫ßn ƒë√¢y)
    const recentMessages = conversation
      .slice(-5) // L·∫•y 5 message g·∫ßn nh·∫•t
      .filter(msg => msg.text && msg.text !== "[ƒêang x·ª≠ l√Ω...]")
      .map(msg => ({
        speaker: msg.type === 'user' ? 0 : 1,
        text: msg.text
      }));
    
    // Lu√¥n th·ª≠ g·ªçi TTS API tr∆∞·ªõc (cho c·∫£ native v√† asian) ƒë·ªÉ s·ª≠ d·ª•ng CSM
    try {
      const response = await api.post('/learners/tts/generate', {
        text: text,
        voiceType: currentVoiceType,
        voiceOrigin: currentVoiceOrigin,
        region: currentVoiceOrigin === 'asian' ? 'north' : undefined, // Ch·ªâ c·∫ßn region cho asian
        useCSM: true, // B·∫≠t CSM ƒë·ªÉ backend th·ª≠ CSM tr∆∞·ªõc
        context: recentMessages // G·ª≠i context cho CSM
      }, {
        timeout: 60000 // 60 gi√¢y timeout (cho l·∫ßn load model ƒë·∫ßu ti√™n)
      });

      if (response.data.success && response.data.audioBase64) {
        // Ph√°t audio t·ª´ base64 (t·ª´ CSM ho·∫∑c FPT.AI)
        const audioData = Uint8Array.from(atob(response.data.audioBase64), c => c.charCodeAt(0));
        const blob = new Blob([audioData], { type: response.data.mimeType || 'audio/wav' });
        const audioUrl = URL.createObjectURL(blob);
        
        // L∆∞u audio URL ƒë·ªÉ c√≥ th·ªÉ ph√°t l·∫°i sau
        setAiAudioUrls(prev => ({ ...prev, [messageId]: audioUrl }));
        
        const audio = new Audio(audioUrl);
        audio.onplay = () => setPlayingAudio(messageId);
        audio.onended = () => {
          setPlayingAudio(null);
          // KH√îNG t·ª± ƒë·ªông b·∫Øt ƒë·∫ßu recording n·ªØa - ng∆∞·ªùi d√πng ph·∫£i click v√†o mic
          const source = response.data.source || 'unknown';
          console.log(`‚úÖ AI finished speaking (${source === 'csm' ? 'CSM' : source === 'fpt' ? 'FPT.AI' : 'TTS'}) - mic will NOT auto-start`);
        };
        audio.onerror = () => {
          setPlayingAudio(null);
          // Fallback v·ªÅ SpeechSynthesis n·∫øu audio fail
          console.warn("‚ö†Ô∏è Audio playback error, falling back to browser TTS");
          speakTextWithBrowser(text, messageId);
        };
        
        audio.play();
        return;
      } else if (response.data.fallback) {
        // Backend tr·∫£ v·ªÅ fallback flag, d√πng browser TTS
        console.log("‚ÑπÔ∏è TTS API unavailable, using browser TTS");
        speakTextWithBrowser(text, messageId);
        return;
      }
    } catch (err) {
      // N·∫øu API fail (timeout, network error, etc.), fallback v·ªÅ browser TTS
      console.warn("‚ö†Ô∏è TTS API error, falling back to browser TTS:", err.message);
    }
    
    // Fallback: D√πng SpeechSynthesis cho browser n·∫øu API kh√¥ng available
    speakTextWithBrowser(text, messageId);
  };

  // H√†m ph·ª• ƒë·ªÉ d√πng SpeechSynthesis API c·ªßa browser
  const speakTextWithBrowser = (text, messageId) => {
    // X·ª≠ l√Ω text cho gi·ªçng n·ªØ Vi·ªát Nam: th√™m pause nh·∫π gi·ªØa c√°c c√¢u ƒë·ªÉ t·ª± nhi√™n h∆°n
    let processedText = text;
    if (voiceType === 'female' && voiceOrigin === 'asian') {
      // Th√™m kho·∫£ng d·ª´ng ng·∫Øn sau d·∫•u ch·∫•m, d·∫•u ph·∫©y ƒë·ªÉ tr√≤n v√†nh r√µ ch·ªØ
      processedText = text
        .replace(/\./g, '. ') // Th√™m space sau d·∫•u ch·∫•m
        .replace(/,/g, ', ') // Th√™m space sau d·∫•u ph·∫©y
        .replace(/\?/g, '? ') // Th√™m space sau d·∫•u h·ªèi
        .replace(/!/g, '! '); // Th√™m space sau d·∫•u ch·∫•m than
    }
    
    // V·ªõi SpeechSynthesis, kh√¥ng th·ªÉ l∆∞u audio URL tr·ª±c ti·∫øp
    // Nh∆∞ng ch√∫ng ta v·∫´n c√≥ th·ªÉ track khi n√†o ƒëang ph√°t
    const utterance = new SpeechSynthesisUtterance(processedText);
    const voices = window.speechSynthesis.getVoices();
    
    if (voiceType === 'male') {
      if (voiceOrigin === 'native') {
        // Gi·ªçng nam b·∫£n ƒë·ªãa (ti·∫øng Anh b·∫£n ƒë·ªãa)
        utterance.lang = 'en-US';
        utterance.rate = 0.85;
        utterance.pitch = 0.85; // Tr·∫ßm
        utterance.volume = 1.0;
        
        const maleVoice = voices.find(voice => 
          voice.lang.includes('en-US') && 
          (voice.name.toLowerCase().includes('david') ||
           voice.name.toLowerCase().includes('mark') ||
           voice.name.toLowerCase().includes('richard') ||
           voice.gender === 'male')
        ) || voices.find(voice => 
          voice.lang.includes('en-US') && 
          !voice.name.toLowerCase().includes('female') &&
          !voice.name.toLowerCase().includes('zira')
        );
        
        if (maleVoice) utterance.voice = maleVoice;
      } else {
        // Gi·ªçng nam ch√¢u √Å (ti·∫øng Anh v·ªõi accent ch√¢u √Å)
        utterance.lang = 'en-US';
        utterance.rate = 0.8;
        utterance.pitch = 0.9; // H∆°i cao h∆°n m·ªôt ch√∫t
        utterance.volume = 0.95;
        
        // T√¨m voice nam c√≥ accent ch√¢u √Å ho·∫∑c ti·∫øng Anh ch√¢u √Å
        const asianMaleVoice = voices.find(voice => 
          (voice.lang.includes('en') || voice.lang.includes('vi')) && 
          (voice.name.toLowerCase().includes('male') ||
           voice.gender === 'male')
        ) || voices.find(voice => 
          voice.lang.includes('en-US')
        );
        
        if (asianMaleVoice) utterance.voice = asianMaleVoice;
      }
    } else {
      // Gi·ªçng n·ªØ
      if (voiceOrigin === 'native') {
        // Gi·ªçng n·ªØ b·∫£n ƒë·ªãa (ti·∫øng Anh b·∫£n ƒë·ªãa)
        utterance.lang = 'en-US';
        utterance.rate = 0.8;
        utterance.pitch = 1.1;
        utterance.volume = 0.9;
        
        const femaleVoice = voices.find(voice => 
          voice.lang.includes('en-US') && 
          (voice.name.toLowerCase().includes('zira') ||
           voice.name.toLowerCase().includes('samantha') ||
           voice.name.toLowerCase().includes('karen') ||
           voice.name.toLowerCase().includes('susan') ||
           voice.gender === 'female')
        ) || voices.find(voice => 
          voice.lang.includes('en-US') && 
          !voice.name.toLowerCase().includes('male')
        );
        
        if (femaleVoice) utterance.voice = femaleVoice;
      } else {
        // Gi·ªçng n·ªØ ch√¢u √Å (ti·∫øng Vi·ªát - gi·ªçng ng∆∞·ªùi Vi·ªát Nam)
        // ƒê·∫∑c ƒëi·ªÉm: √™m √°i, chu·∫©n m·ª±c, ƒë·∫ßy bi·ªÉu c·∫£m, tr√≤n v√†nh r√µ ch·ªØ
        utterance.lang = 'vi-VN';
        utterance.rate = 0.75; // Ch·∫≠m h∆°n ƒë·ªÉ tr√≤n v√†nh r√µ ch·ªØ, d·ªÖ nghe
        utterance.pitch = 1.2; // Cao h∆°n m·ªôt ch√∫t ƒë·ªÉ c√≥ √¢m s·∫Øc bi·ªÉu c·∫£m, √™m √°i
        utterance.volume = 0.85; // Nh·∫π nh√†ng, √™m √°i h∆°n
        
        // T√¨m voice n·ªØ ti·∫øng Vi·ªát - ∆∞u ti√™n voice c√≥ t√™n ch·ª©a "female", "n·ªØ", ho·∫∑c gender = female
        const vietnameseFemaleVoice = voices.find(voice => 
          voice.lang.includes('vi') && 
          (voice.name.toLowerCase().includes('female') || 
           voice.name.toLowerCase().includes('n·ªØ') ||
           voice.gender === 'female')
        ) || voices.find(voice => 
          voice.lang.includes('vi-VN') &&
          !voice.name.toLowerCase().includes('male')
        ) || voices.find(voice => 
          voice.lang.includes('vi') &&
          !voice.name.toLowerCase().includes('male')
        );
        
        if (vietnameseFemaleVoice) {
          utterance.voice = vietnameseFemaleVoice;
        }
        
        // Th√™m pause nh·∫π gi·ªØa c√°c c√¢u ƒë·ªÉ t·ª± nhi√™n h∆°n (th√¥ng qua text processing)
        // S·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω trong text tr∆∞·ªõc khi speak
      }
    }

    utterance.onstart = () => {
      setPlayingAudio(messageId);
    };

    utterance.onend = () => {
      setPlayingAudio(null);
      // KH√îNG t·ª± ƒë·ªông b·∫Øt ƒë·∫ßu recording n·ªØa - ng∆∞·ªùi d√πng ph·∫£i click v√†o mic
      console.log("‚úÖ AI finished speaking - mic will NOT auto-start");
    };

    utterance.onerror = () => {
      setPlayingAudio(null);
    };

    // ƒê·∫£m b·∫£o voices ƒë√£ load tr∆∞·ªõc khi speak (quan tr·ªçng cho gi·ªçng Vi·ªát Nam)
    if (voices.length === 0) {
      // N·∫øu voices ch∆∞a load, ƒë·ª£i m·ªôt ch√∫t r·ªìi th·ª≠ l·∫°i
      window.speechSynthesis.onvoiceschanged = () => {
        const updatedVoices = window.speechSynthesis.getVoices();
        if (updatedVoices.length > 0) {
          // T√¨m l·∫°i voice ph√π h·ª£p v·ªõi voices m·ªõi
          if (voiceType === 'female' && voiceOrigin === 'asian') {
            const vietnameseFemaleVoice = updatedVoices.find(voice => 
              voice.lang.includes('vi') && 
              (voice.name.toLowerCase().includes('female') || 
               voice.name.toLowerCase().includes('n·ªØ') ||
               voice.gender === 'female')
            ) || updatedVoices.find(voice => 
              voice.lang.includes('vi-VN') &&
              !voice.name.toLowerCase().includes('male')
            );
            
            if (vietnameseFemaleVoice) {
              utterance.voice = vietnameseFemaleVoice;
            }
          }
          window.speechSynthesis.speak(utterance);
        }
      };
    } else {
      window.speechSynthesis.speak(utterance);
    }
  };

  // Play audio segment t·ª´ word ƒë·∫øn h·∫øt (gi·ªëng challenge)
  const playSegment = (messageId, start, end, wordIdx) => {
    const audio = userAudioRefs.current[messageId];
    if (!audio) {
      console.warn("Audio not found for message:", messageId, "Available refs:", Object.keys(userAudioRefs.current));
      return;
    }
    
    if (typeof start !== "number" || typeof end !== "number") {
      console.warn("Invalid start/end time:", { start, end });
      return;
    }
    
    console.log("Playing segment:", { messageId, start, end, wordIdx, audioSrc: audio.src });
    
    // ƒê·∫£m b·∫£o audio ƒë√£ ƒë∆∞·ª£c load
    const tryPlay = () => {
      if (audio.readyState >= 2) {
        const startTime = start > 1000 ? start / 1000 : start;
        
        // Pause tr∆∞·ªõc ƒë·ªÉ reset
        audio.pause();
        
        // Set currentTime v√† ph√°t t·ª´ ƒë√≥ ƒë·∫øn h·∫øt
        const onSeeked = () => {
          const playPromise = audio.play();
          if (playPromise !== undefined) {
            playPromise
              .then(() => {
                console.log("Audio playing from", audio.currentTime);
              })
              .catch(error => {
                console.error("Audio play error:", error);
              });
          }
          audio.removeEventListener('seeked', onSeeked);
        };
        
        audio.addEventListener('seeked', onSeeked, { once: true });
        audio.currentTime = Math.max(0, startTime);
        console.log("Setting currentTime to:", startTime);
        
        // Fallback: n·∫øu seeked kh√¥ng fire trong 500ms, th·ª≠ ph√°t lu√¥n
        setTimeout(() => {
          if (audio.paused && Math.abs(audio.currentTime - startTime) < 0.1) {
            console.log("Seeked event timeout, playing anyway");
            onSeeked();
          }
        }, 500);
      } else {
        console.log("Audio not ready, waiting...", audio.readyState);
        setTimeout(tryPlay, 100);
      }
    };
    
    if (audio.readyState === 0) {
      audio.load();
      audio.addEventListener('loadeddata', tryPlay, { once: true });
    } else {
      tryPlay();
    }
    
    // Scroll ƒë·∫øn t·ª´ ƒë∆∞·ª£c click
    setTimeout(() => {
      const el = document.querySelector(`[data-word-idx="${wordIdx}"][data-message-id="${messageId}"]`);
      if (el) {
        el.scrollIntoView({ behavior: "smooth", block: "center", inline: "center" });
      }
    }, 100);
  };

  // Fetch word definition t·ª´ dictionary API
  const fetchWordDefinition = async (word) => {
    if (loadingWord) return;
    
    setLoadingWord(true);
    setOpenWordTooltip(word);
    
    try {
      const res = await api.get(`/learners/dictionary/${encodeURIComponent(word)}`);
      setWordTooltip(res.data);
    } catch (err) {
      console.error("‚ùå Error fetching word definition:", err);
      setWordTooltip({ word, definition: "Kh√¥ng t√¨m th·∫•y nghƒ©a c·ªßa t·ª´ n√†y." });
    } finally {
      setLoadingWord(false);
    }
  };

  // Render transcript v·ªõi words c√≥ th·ªÉ click ƒë·ªÉ ph√°t audio (gi·ªëng challenge)
  const renderTranscriptWithWords = (msg) => {
    if (!msg.transcriptJson || !msg.audioUrl) {
      // Fallback: render text th√¥ng th∆∞·ªùng
      return <p style={{ margin: 0, color: "white" }}>{msg.text}</p>;
    }
    
    const words = msg.transcriptJson.words || [];
    if (words.length === 0) {
      return <p style={{ margin: 0, color: "white" }}>{msg.text}</p>;
    }
    
    const messageId = msg.id || `user-${msg.timestamp?.getTime() || Date.now()}`;
    
    return (
      <div style={{ 
        display: "flex", 
        flexWrap: "wrap", 
        gap: "6px", 
        lineHeight: 1.8,
        alignItems: "center"
      }}>
        {words.map((w, idx) => {
          const wordText = String(w.word || "").toLowerCase().trim();
          if (!wordText) return null;
          
          const playStart = typeof w.start === "number" ? w.start : 0;
          const playEnd = typeof w.end === "number" ? w.end : (playStart + 2);
          
          return (
            <button
              key={idx}
              type="button"
              data-word-idx={idx}
              data-message-id={messageId}
              onClick={(e) => {
                e.stopPropagation();
                playSegment(messageId, playStart, playEnd, idx);
              }}
              style={{
                background: "rgba(255, 255, 255, 0.25)",
                border: "1px solid rgba(255, 255, 255, 0.3)",
                borderRadius: 6,
                padding: "4px 8px",
                color: "white",
                cursor: "pointer",
                fontSize: 15,
                fontWeight: 500,
                transition: "all 0.2s ease",
                textTransform: "lowercase",
                whiteSpace: "nowrap",
                boxShadow: "0 2px 4px rgba(0, 0, 0, 0.1)"
              }}
              onMouseEnter={(e) => {
                e.target.style.background = "rgba(255, 255, 255, 0.4)";
                e.target.style.borderColor = "rgba(255, 255, 255, 0.5)";
                e.target.style.transform = "translateY(-2px)";
                e.target.style.boxShadow = "0 4px 8px rgba(0, 0, 0, 0.15)";
              }}
              onMouseLeave={(e) => {
                e.target.style.background = "rgba(255, 255, 255, 0.25)";
                e.target.style.borderColor = "rgba(255, 255, 255, 0.3)";
                e.target.style.transform = "translateY(0)";
                e.target.style.boxShadow = "0 2px 4px rgba(0, 0, 0, 0.1)";
              }}
            >
              {wordText}
            </button>
          );
        })}
      </div>
    );
  };

  // T·∫°o word timings gi·∫£ cho AI message d·ª±a tr√™n text v√† audio duration
  // T√≠nh to√°n d·ª±a tr√™n v·ªã tr√≠ c·ªßa t·ª´ trong text (t·ª∑ l·ªá)
  const createAIWordTimings = (text, audioDuration) => {
    if (!text || !audioDuration || audioDuration <= 0) return [];
    
    // T√°ch text th√†nh words v√† gi·ªØ l·∫°i v·ªã tr√≠ trong text g·ªëc
    const words = [];
    const regex = /\b\w+\b/g;
    let match;
    let lastIndex = 0;
    
    while ((match = regex.exec(text)) !== null) {
      const word = match[0];
      const startPos = match.index; // V·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa t·ª´ trong text
      const endPos = match.index + word.length; // V·ªã tr√≠ k·∫øt th√∫c
      
      words.push({
        word: word,
        startPos: startPos,
        endPos: endPos
      });
    }
    
    if (words.length === 0) return [];
    
    const totalLength = text.length;
    const wordsWithTimings = [];
    
    words.forEach((wordInfo, idx) => {
      // T√≠nh th·ªùi gian d·ª±a tr√™n v·ªã tr√≠ c·ªßa t·ª´ trong text (t·ª∑ l·ªá)
      // startPos / totalLength = t·ª∑ l·ªá v·ªã tr√≠ trong text
      // T·ª∑ l·ªá n√†y t∆∞∆°ng ·ª©ng v·ªõi th·ªùi gian trong audio
      const startRatio = wordInfo.startPos / totalLength;
      const endRatio = wordInfo.endPos / totalLength;
      
      const start = startRatio * audioDuration;
      const end = endRatio * audioDuration;
      
      wordsWithTimings.push({
        word: wordInfo.word,
        start: start,
        end: end
      });
    });
    
    console.log("Created word timings:", wordsWithTimings.slice(0, 5), "...");
    return wordsWithTimings;
  };

  // Play AI audio t·ª´ word ƒë·∫øn h·∫øt (gi·ªëng challenge detail)
  const playAIAudioFromWord = (messageId, wordIndex, text) => {
    console.log("playAIAudioFromWord called:", { messageId, wordIndex, text, availableRefs: Object.keys(userAudioRefs.current) });
    
    const audio = userAudioRefs.current[`ai-${messageId}`] || userAudioRefs.current[messageId];
    if (!audio) {
      console.warn("AI Audio not found for message:", messageId, "Available refs:", Object.keys(userAudioRefs.current));
      // Th·ª≠ t√¨m l·∫°i sau m·ªôt ch√∫t
      setTimeout(() => {
        const retryAudio = userAudioRefs.current[`ai-${messageId}`] || userAudioRefs.current[messageId];
        if (retryAudio) {
          playAIAudioFromWord(messageId, wordIndex, text);
        }
      }, 200);
      return;
    }
    
    console.log("Audio found:", { src: audio.src, duration: audio.duration, readyState: audio.readyState });
    
    // L·∫•y audio duration
    const audioDuration = audio.duration || 0;
    if (!audioDuration || isNaN(audioDuration)) {
      console.log("Audio duration not available, waiting for loadedmetadata...");
      // N·∫øu ch∆∞a c√≥ duration, ƒë·ª£i loadedmetadata
      const onLoadedMetadata = () => {
        console.log("Audio metadata loaded, duration:", audio.duration);
        const words = createAIWordTimings(text, audio.duration);
        console.log("Created word timings:", words);
        if (words[wordIndex]) {
          playAIAudioSegment(messageId, words[wordIndex].start, audio.duration, wordIndex);
        } else {
          // Fallback: ph√°t t·ª´ ƒë·∫ßu
          playAIAudio(messageId);
        }
      };
      
      if (audio.readyState >= 1) {
        // Metadata ƒë√£ c√≥, th·ª≠ l·∫•y duration
        if (audio.duration && !isNaN(audio.duration)) {
          onLoadedMetadata();
        } else {
          audio.addEventListener('loadedmetadata', onLoadedMetadata, { once: true });
          audio.load();
        }
      } else {
        audio.addEventListener('loadedmetadata', onLoadedMetadata, { once: true });
        audio.load();
      }
      return;
    }
    
    // T·∫°o word timings
    const words = createAIWordTimings(text, audioDuration);
    console.log("Word timings:", words, "wordIndex:", wordIndex);
    if (words[wordIndex]) {
      playAIAudioSegment(messageId, words[wordIndex].start, audioDuration, wordIndex);
    } else {
      // Fallback: ph√°t t·ª´ ƒë·∫ßu
      console.log("Word index not found, playing from start");
      playAIAudio(messageId);
    }
  };

  // Play AI audio segment t·ª´ start ƒë·∫øn h·∫øt (gi·ªëng ChallengeDetail)
  const playAIAudioSegment = (messageId, start, end, wordIdx) => {
    const audio = userAudioRefs.current[`ai-${messageId}`] || userAudioRefs.current[messageId];
    if (!audio) {
      console.warn("Audio not found for message:", messageId);
      return;
    }
    
    if (typeof start !== "number" || isNaN(start)) {
      console.warn("Invalid start time:", start);
      return;
    }
    
    // ƒê·∫£m b·∫£o start l√† s·ªë gi√¢y (kh√¥ng ph·∫£i milliseconds)
    const startTime = start > 1000 ? start / 1000 : start;
    
    console.log("Playing AI audio segment:", { messageId, startTime, audioDuration: audio.duration, readyState: audio.readyState });
    
    // ƒê·∫£m b·∫£o audio ƒë√£ ƒë∆∞·ª£c load
    const tryPlay = () => {
      if (audio.readyState >= 2) { // HAVE_CURRENT_DATA ho·∫∑c cao h∆°n
        // Pause tr∆∞·ªõc ƒë·ªÉ reset
        audio.pause();
        
        // Set currentTime v√† ƒë·ª£i seeked event ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√£ seek xong
        const onSeeked = () => {
          // Ph√°t audio sau khi ƒë√£ seek xong - ph√°t ti·∫øp ƒë·∫øn h·∫øt, kh√¥ng d·ª´ng
          const playPromise = audio.play();
          if (playPromise !== undefined) {
            playPromise
              .then(() => {
                console.log("‚úÖ AI audio playing from", audio.currentTime, "- will continue to end");
                setPlayingAudio(messageId);
              })
              .catch(error => {
                console.error("‚ùå AI Audio play error:", error);
                setPlayingAudio(null);
              });
          }
          audio.removeEventListener('seeked', onSeeked);
        };
        
        audio.addEventListener('seeked', onSeeked, { once: true });
        audio.currentTime = Math.max(0, startTime);
        console.log("Setting currentTime to:", startTime, "seconds");
        
        // Fallback: n·∫øu seeked kh√¥ng fire trong 500ms, th·ª≠ ph√°t lu√¥n
        setTimeout(() => {
          if (audio.paused && Math.abs(audio.currentTime - startTime) < 0.1) {
            console.log("Seeked event timeout, playing anyway");
            onSeeked();
          }
        }, 500);
      } else {
        // N·∫øu ch∆∞a load xong, ƒë·ª£i m·ªôt ch√∫t r·ªìi th·ª≠ l·∫°i
        console.log("Audio not ready, waiting...", audio.readyState);
        setTimeout(tryPlay, 100);
      }
    };
    
    // Th·ª≠ ph√°t ngay ho·∫∑c ƒë·ª£i audio load
    if (audio.readyState === 0) {
      // N·∫øu ch∆∞a load, load tr∆∞·ªõc
      audio.load();
      audio.addEventListener('loadeddata', tryPlay, { once: true });
    } else {
      tryPlay();
    }
  };

  // Play AI audio t·ª´ ƒë·∫ßu (fallback)
  const playAIAudio = (messageId) => {
    const audio = userAudioRefs.current[`ai-${messageId}`] || userAudioRefs.current[messageId];
    if (!audio) {
      console.warn("AI Audio not found for message:", messageId);
      return;
    }
    
    // Ph√°t t·ª´ ƒë·∫ßu
    audio.currentTime = 0;
    const playPromise = audio.play();
    if (playPromise !== undefined) {
      playPromise
        .then(() => {
          console.log("AI audio playing from start");
          setPlayingAudio(messageId);
        })
        .catch(error => {
          console.error("AI Audio play error:", error);
        });
    }
  };

  // Render text v·ªõi clickable words cho dictionary (cho AI messages)
  const renderTextWithDictionary = (text, messageId) => {
    if (!text) return null;
    
    // Split text th√†nh words v√† spaces, gi·ªØ l·∫°i index c·ªßa t·ª´
    const parts = text.split(/(\s+|[.,!?;:])/);
    let wordIndex = -1; // Index c·ªßa t·ª´ (kh√¥ng t√≠nh spaces v√† punctuation)
    
    return (
      <>
        {parts.map((part, idx) => {
          // N·∫øu l√† kho·∫£ng tr·∫Øng ho·∫∑c punctuation, render tr·ª±c ti·∫øp
          if (/^\s+$/.test(part) || /^[.,!?;:]+$/.test(part)) {
            return <span key={idx}>{part}</span>;
          }
          
          // Clean word (remove punctuation)
          const cleanWord = part.replace(/[.,!?;:]/g, "").toLowerCase();
          
          // Ch·ªâ l√†m clickable n·∫øu l√† t·ª´ ti·∫øng Anh (c√≥ ch·ªØ c√°i)
          if (cleanWord.match(/^[a-z]+$/i) && cleanWord.length > 1) {
            wordIndex++; // TƒÉng index cho t·ª´
            const currentWordIndex = wordIndex; // L∆∞u l·∫°i ƒë·ªÉ d√πng trong onClick
            
            return (
              <WordTooltipWrapper
                key={idx}
                word={part}
                cleanWord={cleanWord}
                isOpen={openWordTooltip === cleanWord}
                tooltip={wordTooltip}
                loading={loadingWord}
                micButtonRef={micButtonRef}
                onWordClick={() => {
                  // Click v√†o t·ª´ ch·ªâ ƒë·ªÉ m·ªü tooltip - KH√îNG ph√°t audio
                  // Audio ch·ªâ ph√°t khi click v√†o n√∫t ph√°t l·∫°i b√™n c·∫°nh timestamp
                  fetchWordDefinition(cleanWord);
                }}
                onClose={() => {
                  setOpenWordTooltip(null);
                  setWordTooltip(null);
                }}
              />
            );
          }
          
          return <span key={idx}>{part}</span>;
        })}
      </>
    );
  };

  // B·∫Øt ƒë·∫ßu ghi √¢m v·ªõi speech recognition ƒë·ªÉ b·ªè qua t·∫°p √¢m
  const startRecording = async () => {
    // D·ª´ng TTS n·∫øu ƒëang ph√°t (cho ph√©p c·∫Øt ngang)
    if (playingAudio) {
      window.speechSynthesis.cancel();
      setPlayingAudio(null);
    }
    
    // N·∫øu ƒëang recording, kh√¥ng l√†m g√¨ (ƒë√£ x·ª≠ l√Ω trong onClick)
    if (isRecording) {
      return;
    }
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStreamRef.current = stream;
      audioChunksRef.current = [];
      lastSoundTimeRef.current = Date.now();

      const options = 
        typeof MediaRecorder !== "undefined" &&
        MediaRecorder.isTypeSupported &&
        MediaRecorder.isTypeSupported("audio/webm")
          ? { mimeType: "audio/webm" }
          : undefined;

      const mr = new MediaRecorder(stream, options);
      mediaRecorderRef.current = mr;

      mr.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          audioChunksRef.current.push(e.data);
          lastSoundTimeRef.current = Date.now(); // C·∫≠p nh·∫≠t khi c√≥ data
        }
      };

      mr.onstop = () => {
        const blob = new Blob(audioChunksRef.current, { 
          type: audioChunksRef.current[0]?.type || "audio/webm" 
        });
        
        console.log("üé§ Recording stopped:", {
          blobSize: blob?.size,
          chunksCount: audioChunksRef.current.length,
          chunksSizes: audioChunksRef.current.map(chunk => chunk.size),
          blobType: blob?.type
        });
        
        // Lu√¥n t·ª± ƒë·ªông g·ª≠i khi d·ª´ng recording (n·∫øu c√≥ audio)
        // Gi·∫£m threshold xu·ªëng 100 bytes ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c c·∫£ audio ng·∫Øn
        if (blob && blob.size > 100) { // √çt nh·∫•t 100 bytes (gi·∫£m t·ª´ 500)
          console.log("‚úÖ Audio recorded, sending to server. Size:", blob.size, "bytes");
          handleAudioRecorded(blob);
        } else {
          // N·∫øu kh√¥ng c√≥ audio, reset
          console.warn("‚ö†Ô∏è Audio too small or empty. Size:", blob?.size, "bytes. Not sending.");
          audioChunksRef.current = [];
          setHasRecordedAudio(false);
        }
        
        // Stop all tracks
        if (mediaStreamRef.current) {
          mediaStreamRef.current.getTracks().forEach(track => track.stop());
          mediaStreamRef.current = null;
        }
      };

      mr.start();
      setIsRecording(true);

      // B·∫Øt ƒë·∫ßu Web Speech API ƒë·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i v√† b·ªè qua t·∫°p √¢m
      startSpeechRecognition();
      
      // B·∫Øt ƒë·∫ßu ph√°t hi·ªán im l·∫∑ng ƒë·ªÉ t·ª± ƒë·ªông g·ª≠i
      startSilenceDetection();
    } catch (err) {
      console.error("‚ùå Error starting recording:", err);
      alert("Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng ki·ªÉm tra quy·ªÅn truy c·∫≠p.");
    }
  };

  // Web Speech API ƒë·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i v√† c·∫≠p nh·∫≠t lastSoundTime
  const startSpeechRecognition = () => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      return; // Browser kh√¥ng h·ªó tr·ª£
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    recognitionRef.current = recognition;

    recognition.continuous = true;
    recognition.interimResults = false;
    recognition.lang = 'en-US';

    recognition.onresult = (event) => {
      // Khi c√≥ k·∫øt qu·∫£ nh·∫≠n di·ªán gi·ªçng n√≥i, c·∫≠p nh·∫≠t th·ªùi gian
      lastSoundTimeRef.current = Date.now();
    };

    recognition.onerror = (event) => {
      // B·ªè qua l·ªói "aborted" v√† c√°c l·ªói kh√¥ng quan tr·ªçng
      if (event.error !== 'aborted' && event.error !== 'no-speech') {
        console.warn("Speech recognition error:", event.error);
      }
    };

    recognition.onend = () => {
      // T·ª± ƒë·ªông restart n·∫øu v·∫´n ƒëang recording
      if (isRecording && recognitionRef.current) {
        try {
          recognitionRef.current.start();
        } catch (e) {
          // Ignore restart errors
        }
      }
    };

    try {
      recognition.start();
    } catch (e) {
      // Ignore start errors
    }
  };

  // Ph√°t hi·ªán im l·∫∑ng ƒë·ªÉ t·ª± ƒë·ªông g·ª≠i (sau 2 gi√¢y im l·∫∑ng)
  const startSilenceDetection = () => {
    const SILENCE_THRESHOLD = 2000; // 2 gi√¢y im l·∫∑ng
    
    const checkSilence = () => {
      if (!isRecording) return;
      
      const timeSinceLastSound = Date.now() - lastSoundTimeRef.current;
      
      // N·∫øu c√≥ audio chunks v√† ƒë√£ im l·∫∑ng ƒë·ªß l√¢u, t·ª± ƒë·ªông g·ª≠i
      if (audioChunksRef.current.length > 0 && timeSinceLastSound >= SILENCE_THRESHOLD) {
        stopRecording();
        return;
      }
      
      // Ti·∫øp t·ª•c ki·ªÉm tra
      silenceTimerRef.current = setTimeout(checkSilence, 500);
    };
    
    checkSilence();
  };

  // D·ª´ng ghi √¢m
  const stopRecording = () => {
    // ƒê·∫∑t isRecording = false ngay ƒë·ªÉ tr√°nh c√°c logic kh√°c ch·∫°y
    setIsRecording(false);
    
    // D·ª´ng speech recognition
    if (recognitionRef.current) {
      try {
        recognitionRef.current.abort(); // D√πng abort thay v√¨ stop ƒë·ªÉ tr√°nh l·ªói
        recognitionRef.current = null;
      } catch (e) {
        // Ignore stop errors
      }
    }
    
    // D·ª´ng silence detection
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    // D·ª´ng MediaRecorder
    if (mediaRecorderRef.current) {
      try {
        if (mediaRecorderRef.current.state === "recording") {
          mediaRecorderRef.current.stop();
        } else if (mediaRecorderRef.current.state === "paused") {
          mediaRecorderRef.current.stop();
        }
      } catch (e) {
        console.warn("Error stopping MediaRecorder:", e);
      }
    }
    
    // Stop all tracks
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => {
        track.stop();
      });
      mediaStreamRef.current = null;
    }
  };


  // X·ª≠ l√Ω audio ƒë√£ ghi
  const handleAudioRecorded = (blob) => {
    // Ch·ªâ t·ª± ƒë·ªông g·ª≠i n·∫øu c√≥ audio data
    console.log("üì§ handleAudioRecorded called:", {
      blobSize: blob?.size,
      blobType: blob?.type,
      hasBlob: !!blob
    });
    
    if (blob && blob.size > 0) {
      sendMessage(null, blob);
    } else {
      console.warn("‚ö†Ô∏è handleAudioRecorded: Invalid blob, not sending");
    }
  };

  // G·ª≠i message (ch·ªâ audio)
  const sendMessage = async (text = null, audio = null) => {
    if (!audio) {
      console.warn("‚ö†Ô∏è sendMessage: No audio provided");
      return;
    }

    console.log("üì® sendMessage called:", {
      hasAudio: !!audio,
      audioSize: audio.size,
      audioType: audio.type,
      sessionId: sessionId
    });

    const userMessage = {
      type: "user",
      text: "[ƒêang x·ª≠ l√Ω...]",
      audio: audio,
      timestamp: new Date(),
      id: `user-${Date.now()}`
    };

    setConversation(prev => [...prev, userMessage]);
    setSending(true);

    try {
      const formData = new FormData();
      if (audio) {
        // ƒê·∫£m b·∫£o audio l√† File object, kh√¥ng ph·∫£i Blob
        const audioFile = audio instanceof File 
          ? audio 
          : new File([audio], `recording-${Date.now()}.webm`, { type: audio.type || "audio/webm" });
        formData.append("audio", audioFile);
        console.log("üìé Appended audio to FormData:", {
          fileName: audioFile.name,
          fileSize: audioFile.size,
          fileType: audioFile.type
        });
      }
      formData.append("session_id", sessionId);

      console.log("üöÄ Sending audio to server...");
      const res = await api.post(
        "/learners/speaking-practice/story/message",
        formData,
        { headers: { "Content-Type": "multipart/form-data" } }
      );

      console.log("‚úÖ Server response received:", {
        hasTranscript: !!res.data.transcript,
        hasTranscriptJson: !!res.data.transcriptJson,
        hasResponse: !!res.data.response
      });

      // T·∫°o audio URL tr∆∞·ªõc khi c·∫≠p nh·∫≠t message
      const audioUrl = URL.createObjectURL(audio);
      
      // C·∫≠p nh·∫≠t user message v·ªõi transcript v√† transcriptJson n·∫øu c√≥
      const updatedUserMessage = {
        ...userMessage,
        text: res.data.transcript || "[Audio message]",
        transcriptJson: res.data.transcriptJson || null, // Full transcript v·ªõi words v√† timings
        audioUrl: audioUrl, // L∆∞u audio URL ƒë·ªÉ ph√°t l·∫°i
        audio: audio // Gi·ªØ l·∫°i audio blob
      };
      
      console.log("‚úÖ Updated user message:", {
        id: updatedUserMessage.id,
        hasAudioUrl: !!updatedUserMessage.audioUrl,
        hasTranscriptJson: !!updatedUserMessage.transcriptJson,
        wordsCount: updatedUserMessage.transcriptJson?.words?.length || 0,
        audioUrl: updatedUserMessage.audioUrl
      });

      const aiMessage = {
        type: "ai",
        text: res.data.response,
        timestamp: new Date(),
        id: `ai-${Date.now()}`
      };

      setConversation(prev => {
        const newConv = [...prev];
        const userIdx = newConv.length - 1;
        newConv[userIdx] = updatedUserMessage;
        return [...newConv, aiMessage];
      });

      // Reset audio chunks sau khi g·ª≠i th√†nh c√¥ng
      audioChunksRef.current = [];
      setHasRecordedAudio(false);
      
      // C·∫≠p nh·∫≠t last activity khi c√≥ message m·ªõi
      lastActivityRef.current = Date.now();

      // T·ª± ƒë·ªông ph√°t √¢m AI response (KH√îNG t·ª± ƒë·ªông m·ªü mic sau khi n√≥i xong)
      setTimeout(() => {
        speakText(res.data.response, aiMessage.id);
      }, 500);
    } catch (err) {
      console.error("‚ùå Error sending message:", err);
      const errorMessage = {
        type: "ai",
        text: "Xin l·ªói, c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i.",
        timestamp: new Date(),
        id: `error-${Date.now()}`
      };
      setConversation(prev => [...prev, errorMessage]);
      speakText(errorMessage.text, errorMessage.id);
    } finally {
      setSending(false);
    }
  };

  return (
    <div className="tell-me-story" style={{ 
      display: "flex", 
      flexDirection: "column", 
      height: "100vh",
      background: "linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%)",
      position: "relative",
      overflow: "hidden"
    }}>
      {/* Animated background elements */}
      <div style={{
        position: "absolute",
        top: 0,
        left: 0,
        right: 0,
        bottom: 0,
        background: "radial-gradient(circle at 20% 50%, rgba(255, 255, 255, 0.1) 0%, transparent 50%), radial-gradient(circle at 80% 80%, rgba(255, 255, 255, 0.1) 0%, transparent 50%)",
        pointerEvents: "none",
        zIndex: 0
      }}></div>
      <div className="story-header" style={{
        padding: "20px",
        background: "rgba(255, 255, 255, 0.95)",
        boxShadow: "0 2px 10px rgba(0,0,0,0.1)"
      }}>
        <div style={{ display: "flex", justifyContent: "space-between", alignItems: "center" }}>
          <button 
            className="btn-back" 
            onClick={onBack}
            style={{
              padding: "8px 16px",
              background: "#6b7280",
              color: "white",
              border: "none",
              borderRadius: 8,
              cursor: "pointer",
              fontSize: 14
            }}
          >
            ‚Üê Quay l·∫°i
          </button>
          <h2 style={{ margin: 0, color: "#333" }}>Tell me your story</h2>
          <button
            onClick={handleNewConversation}
            style={{
              padding: "8px 16px",
              background: "#10b981",
              color: "white",
              border: "none",
              borderRadius: 8,
              cursor: "pointer",
              fontSize: 14,
              fontWeight: 500,
              transition: "all 0.2s"
            }}
            onMouseEnter={(e) => {
              e.target.style.background = "#059669";
              e.target.style.transform = "translateY(-1px)";
            }}
            onMouseLeave={(e) => {
              e.target.style.background = "#10b981";
              e.target.style.transform = "translateY(0)";
            }}
          >
            T·∫°o cu·ªôc tr√≤ chuy·ªán m·ªõi
          </button>
        </div>
        <div style={{ 
          marginTop: 10, 
          display: "flex", 
          justifyContent: "center", 
          alignItems: "center",
          gap: 8,
          position: "relative"
        }}>
          <p style={{ margin: 0, color: "#666", fontSize: 14 }}>Gi·ªçng AI:</p>
          <div style={{ display: "flex", gap: 4, background: "#f3f4f6", borderRadius: 8, padding: 4, position: "relative" }}>
            <div style={{ position: "relative" }}>
              <button
                onClick={() => setShowVoiceSubmenu(showVoiceSubmenu === 'male' ? null : 'male')}
                style={{
                  padding: "6px 16px",
                  borderRadius: 6,
                  border: "none",
                  background: voiceType === 'male' 
                    ? "linear-gradient(135deg, #3b82f6 0%, #1e40af 100%)"
                    : "transparent",
                  color: voiceType === 'male' ? "white" : "#666",
                  cursor: "pointer",
                  fontSize: 13,
                  fontWeight: voiceType === 'male' ? 600 : 400,
                  transition: "all 0.2s"
                }}
              >
                Nam {voiceType === 'male' && `(${voiceOrigin === 'native' ? 'B·∫£n ƒë·ªãa' : 'Ch√¢u √Å'})`}
              </button>
              {showVoiceSubmenu === 'male' && (
                <div style={{
                  position: "absolute",
                  top: "100%",
                  left: 0,
                  marginTop: 4,
                  background: "white",
                  borderRadius: 8,
                  boxShadow: "0 4px 6px rgba(0, 0, 0, 0.1)",
                  padding: 4,
                  zIndex: 1000,
                  minWidth: 120
                }}>
                  <button
                    onClick={async () => {
                      const newVoiceType = 'male';
                      const newVoiceOrigin = 'native';
                      setVoiceType(newVoiceType);
                      setVoiceOrigin(newVoiceOrigin);
                      setShowVoiceSubmenu(null);
                      // √Åp d·ª•ng ngay gi·ªçng m·ªõi cho message ƒëang ph√°t
                      if (playingAudio) {
                        const currentMsg = conversation.find(m => m.id === playingAudio);
                        if (currentMsg) {
                          window.speechSynthesis.cancel();
                          setPlayingAudio(null);
                          // ƒê·ª£i state update xong r·ªìi m·ªõi speak v·ªõi gi·ªçng m·ªõi
                          setTimeout(() => {
                            speakText(currentMsg.text, currentMsg.id, newVoiceType, newVoiceOrigin);
                          }, 150);
                        }
                      }
                    }}
                    style={{
                      width: "100%",
                      padding: "8px 12px",
                      borderRadius: 6,
                      border: "none",
                      background: voiceType === 'male' && voiceOrigin === 'native' 
                        ? "#3b82f6" 
                        : "transparent",
                      color: voiceType === 'male' && voiceOrigin === 'native' ? "white" : "#666",
                      cursor: "pointer",
                      fontSize: 13,
                      textAlign: "left"
                    }}
                  >
                    B·∫£n ƒë·ªãa
                  </button>
                  <button
                    onClick={async () => {
                      const newVoiceType = 'male';
                      const newVoiceOrigin = 'asian';
                      setVoiceType(newVoiceType);
                      setVoiceOrigin(newVoiceOrigin);
                      setShowVoiceSubmenu(null);
                      if (playingAudio) {
                        const currentMsg = conversation.find(m => m.id === playingAudio);
                        if (currentMsg) {
                          window.speechSynthesis.cancel();
                          setPlayingAudio(null);
                          setTimeout(() => {
                            speakText(currentMsg.text, currentMsg.id, newVoiceType, newVoiceOrigin);
                          }, 150);
                        }
                      }
                    }}
                    style={{
                      width: "100%",
                      padding: "8px 12px",
                      borderRadius: 6,
                      border: "none",
                      background: voiceType === 'male' && voiceOrigin === 'asian' 
                        ? "#3b82f6" 
                        : "transparent",
                      color: voiceType === 'male' && voiceOrigin === 'asian' ? "white" : "#666",
                      cursor: "pointer",
                      fontSize: 13,
                      textAlign: "left"
                    }}
                  >
                    Ch√¢u √Å
                  </button>
                </div>
              )}
            </div>
            <div style={{ position: "relative" }}>
              <button
                onClick={() => setShowVoiceSubmenu(showVoiceSubmenu === 'female' ? null : 'female')}
                style={{
                  padding: "6px 16px",
                  borderRadius: 6,
                  border: "none",
                  background: voiceType === 'female' 
                    ? "linear-gradient(135deg, #ec4899 0%, #be185d 100%)"
                    : "transparent",
                  color: voiceType === 'female' ? "white" : "#666",
                  cursor: "pointer",
                  fontSize: 13,
                  fontWeight: voiceType === 'female' ? 600 : 400,
                  transition: "all 0.2s"
                }}
              >
                N·ªØ {voiceType === 'female' && `(${voiceOrigin === 'native' ? 'B·∫£n ƒë·ªãa' : 'Ch√¢u √Å'})`}
              </button>
              {showVoiceSubmenu === 'female' && (
                <div style={{
                  position: "absolute",
                  top: "100%",
                  left: 0,
                  marginTop: 4,
                  background: "white",
                  borderRadius: 8,
                  boxShadow: "0 4px 6px rgba(0, 0, 0, 0.1)",
                  padding: 4,
                  zIndex: 1000,
                  minWidth: 120
                }}>
                  <button
                    onClick={async () => {
                      const newVoiceType = 'female';
                      const newVoiceOrigin = 'native';
                      setVoiceType(newVoiceType);
                      setVoiceOrigin(newVoiceOrigin);
                      setShowVoiceSubmenu(null);
                      if (playingAudio) {
                        const currentMsg = conversation.find(m => m.id === playingAudio);
                        if (currentMsg) {
                          window.speechSynthesis.cancel();
                          setPlayingAudio(null);
                          setTimeout(() => {
                            speakText(currentMsg.text, currentMsg.id, newVoiceType, newVoiceOrigin);
                          }, 150);
                        }
                      }
                    }}
                    style={{
                      width: "100%",
                      padding: "8px 12px",
                      borderRadius: 6,
                      border: "none",
                      background: voiceType === 'female' && voiceOrigin === 'native' 
                        ? "#ec4899" 
                        : "transparent",
                      color: voiceType === 'female' && voiceOrigin === 'native' ? "white" : "#666",
                      cursor: "pointer",
                      fontSize: 13,
                      textAlign: "left"
                    }}
                  >
                    B·∫£n ƒë·ªãa
                  </button>
                  <button
                    onClick={async () => {
                      const newVoiceType = 'female';
                      const newVoiceOrigin = 'asian';
                      setVoiceType(newVoiceType);
                      setVoiceOrigin(newVoiceOrigin);
                      setShowVoiceSubmenu(null);
                      if (playingAudio) {
                        const currentMsg = conversation.find(m => m.id === playingAudio);
                        if (currentMsg) {
                          window.speechSynthesis.cancel();
                          setPlayingAudio(null);
                          setTimeout(() => {
                            speakText(currentMsg.text, currentMsg.id, newVoiceType, newVoiceOrigin);
                          }, 150);
                        }
                      }
                    }}
                    style={{
                      width: "100%",
                      padding: "8px 12px",
                      borderRadius: 6,
                      border: "none",
                      background: voiceType === 'female' && voiceOrigin === 'asian' 
                        ? "#ec4899" 
                        : "transparent",
                      color: voiceType === 'female' && voiceOrigin === 'asian' ? "white" : "#666",
                      cursor: "pointer",
                      fontSize: 13,
                      textAlign: "left"
                    }}
                  >
                    Ch√¢u √Å
                  </button>
                </div>
              )}
            </div>
          </div>
          {/* Click outside ƒë·ªÉ ƒë√≥ng submenu */}
          {showVoiceSubmenu && (
            <div
              onClick={() => setShowVoiceSubmenu(null)}
              style={{
                position: "fixed",
                top: 0,
                left: 0,
                right: 0,
                bottom: 0,
                zIndex: 999
              }}
            />
          )}
        </div>
        <p style={{ marginTop: 8, color: "#666", textAlign: "center", fontSize: 13 }}>
          Tr√≤ chuy·ªán v·ªõi b·∫°n AI b·∫±ng gi·ªçng n√≥i. Nh·∫•n v√†o mic ƒë·ªÉ n√≥i!
        </p>
      </div>

      <div className="story-conversation" style={{
        flex: 1,
        display: "flex",
        flexDirection: "column",
        overflow: "hidden",
        position: "relative",
        zIndex: 1
      }}>
        <div className="messages-container" style={{
          flex: 1,
          overflowY: "auto",
          padding: "24px 20px",
          background: "rgba(255, 255, 255, 0.08)",
          backdropFilter: "blur(15px)",
          scrollBehavior: "smooth"
        }}>
          {conversation.map((msg, idx) => (
            <div
              key={idx}
              style={{
                display: "flex",
                justifyContent: msg.type === "user" ? "flex-end" : "flex-start",
                marginBottom: 20,
                animation: "fadeIn 0.3s ease-in"
              }}
            >
              <div style={{
                maxWidth: "70%",
                padding: "14px 18px",
                borderRadius: 20,
                background: msg.type === "user" 
                  ? "linear-gradient(135deg, #667eea 0%, #764ba2 100%)"
                  : "linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%)",
                color: msg.type === "user" ? "white" : "#333",
                boxShadow: msg.type === "user"
                  ? "0 4px 12px rgba(102, 126, 234, 0.3)"
                  : "0 4px 12px rgba(0, 0, 0, 0.1)",
                position: "relative",
                border: msg.type === "ai" ? "1px solid rgba(0, 0, 0, 0.05)" : "none"
              }}>
                {/* User message: Audio ref ·∫©n ƒë·ªÉ ph√°t l·∫°i */}
                {msg.type === "user" && msg.audioUrl && (
                  <audio 
                    ref={(el) => {
                      if (el) {
                        const msgId = msg.id || `user-${msg.timestamp?.getTime() || Date.now()}`;
                        userAudioRefs.current[msgId] = el;
                      }
                    }}
                    src={msg.audioUrl}
                    preload="auto"
                    style={{ display: "none" }}
                  />
                )}
                
                {/* User message: Transcript v·ªõi words c√≥ th·ªÉ click */}
                {msg.type === "user" && (
                  <div style={{ 
                    margin: 0, 
                    lineHeight: 1.6,
                    fontSize: 15,
                    fontWeight: 500,
                    wordBreak: "break-word"
                  }}>
                    {msg.text === "[ƒêang x·ª≠ l√Ω...]" ? (
                      <p style={{ margin: 0, color: "rgba(255, 255, 255, 0.8)", fontStyle: "italic" }}>
                        <span style={{ display: "inline-block", animation: "pulse 1.5s infinite" }}>‚óè</span> ƒêang x·ª≠ l√Ω...
                      </p>
                    ) : msg.transcriptJson && msg.audioUrl ? (
                      <div>{renderTranscriptWithWords(msg)}</div>
                    ) : msg.text && msg.text !== "[Audio message]" ? (
                      <p style={{ margin: 0, color: "white" }}>{msg.text}</p>
                    ) : null}
                  </div>
                )}
                
                {/* AI message: Text v·ªõi dictionary */}
                {msg.type === "ai" && (
                  <>
                    {/* Audio ref ·∫©n cho AI message n·∫øu c√≥ */}
                    {aiAudioUrls[msg.id] && (
                      <audio 
                        ref={(el) => {
                          if (el) {
                            const msgId = msg.id;
                            userAudioRefs.current[`ai-${msgId}`] = el;
                            userAudioRefs.current[msgId] = el; // C≈©ng l∆∞u v·ªõi key kh√¥ng c√≥ prefix
                            console.log("AI audio ref set for:", msgId, "src:", el.src);
                            // ƒê·∫£m b·∫£o audio ƒë∆∞·ª£c load
                            el.load();
                          }
                        }}
                        src={aiAudioUrls[msg.id]}
                        preload="auto"
                        style={{ display: "none" }}
                        onLoadedMetadata={() => {
                          console.log("AI audio metadata loaded for:", msg.id, "duration:", userAudioRefs.current[`ai-${msg.id}`]?.duration);
                        }}
                        onPlay={() => setPlayingAudio(msg.id)}
                        onPause={() => {
                          if (playingAudio === msg.id) setPlayingAudio(null);
                        }}
                        onEnded={() => setPlayingAudio(null)}
                      />
                    )}
                    
                    <div style={{ 
                      margin: 0, 
                      lineHeight: 1.6,
                      fontSize: 15,
                      position: "relative",
                      wordBreak: "break-word"
                    }}>
                      {renderTextWithDictionary(msg.text, msg.id || idx)}
                    </div>
                    
                    {/* Speaking indicator */}
                    {playingAudio === msg.id && (
                      <div style={{
                        position: "absolute",
                        top: 12,
                        right: 12,
                        display: "flex",
                        alignItems: "center",
                        gap: 6,
                        background: "rgba(16, 185, 129, 0.15)",
                        padding: "4px 8px",
                        borderRadius: 12,
                        fontSize: 11,
                        color: "#10b981",
                        fontWeight: 600
                      }}>
                        <div style={{
                          width: 6,
                          height: 6,
                          borderRadius: "50%",
                          background: "#10b981",
                          animation: "pulse 1s infinite"
                        }}></div>
                        ƒêang ph√°t
                      </div>
                    )}
                  </>
                )}
                
                {/* Timestamp v·ªõi n√∫t ph√°t l·∫°i (ch·ªâ cho AI messages) */}
                <div style={{
                  display: "flex",
                  alignItems: "center",
                  justifyContent: "flex-end",
                  gap: 8,
                  marginTop: 8
                }}>
                  {/* N√∫t ph√°t l·∫°i nh·ªè nh·ªè (ch·ªâ hi·ªán cho AI messages) */}
                  {msg.type === "ai" && (
                    <button
                      onClick={(e) => {
                        e.stopPropagation();
                        // Ph√°t l·∫°i audio - ∆∞u ti√™n audio URL, n·∫øu kh√¥ng c√≥ th√¨ d√πng SpeechSynthesis
                        const audio = userAudioRefs.current[`ai-${msg.id}`] || userAudioRefs.current[msg.id];
                        if (audio && aiAudioUrls[msg.id]) {
                          // Ph√°t l·∫°i audio t·ª´ blob URL
                          audio.currentTime = 0;
                          const playPromise = audio.play();
                          if (playPromise !== undefined) {
                            playPromise
                              .then(() => {
                                setPlayingAudio(msg.id);
                              })
                              .catch(error => {
                                console.error("Audio play error:", error);
                              });
                          }
                        } else if (msg.text) {
                          // Fallback: d√πng SpeechSynthesis n·∫øu kh√¥ng c√≥ audio URL
                          speakText(msg.text, msg.id);
                        }
                      }}
                      style={{
                        background: "transparent",
                        border: "none",
                        cursor: "pointer",
                        padding: "4px",
                        display: "flex",
                        alignItems: "center",
                        justifyContent: "center",
                        borderRadius: "50%",
                        transition: "all 0.2s",
                        opacity: 0.6,
                        minWidth: "20px",
                        minHeight: "20px"
                      }}
                      onMouseEnter={(e) => {
                        e.currentTarget.style.opacity = "1";
                        e.currentTarget.style.background = "rgba(16, 185, 129, 0.1)";
                      }}
                      onMouseLeave={(e) => {
                        e.currentTarget.style.opacity = "0.6";
                        e.currentTarget.style.background = "transparent";
                      }}
                      title="Ph√°t l·∫°i"
                    >
                      <FaRedo size={12} color="#10b981" />
                    </button>
                  )}
                  <div style={{
                    fontSize: 11,
                    opacity: 0.7,
                    color: msg.type === "user" ? "rgba(255, 255, 255, 0.9)" : "#666"
                  }}>
                    {msg.timestamp.toLocaleTimeString()}
                  </div>
                </div>
              </div>
            </div>
          ))}
          {sending && (
            <div style={{
              display: "flex",
              justifyContent: "flex-start",
              marginBottom: 20
            }}>
              <div style={{
                padding: "12px 16px",
                borderRadius: 18,
                background: "white",
                boxShadow: "0 2px 8px rgba(0,0,0,0.15)"
              }}>
                <div style={{ display: "flex", alignItems: "center", gap: 8 }}>
                  <div style={{
                    width: 8,
                    height: 8,
                    borderRadius: "50%",
                    background: "#10b981",
                    animation: "pulse 1s infinite"
                  }}></div>
                  <span style={{ color: "#666", fontSize: 14 }}>AI ƒëang suy nghƒ©...</span>
                </div>
              </div>
            </div>
          )}
          <div ref={messagesEndRef} />
        </div>

        <div className="story-input" style={{
          padding: "24px 20px",
          background: "rgba(255, 255, 255, 0.98)",
          boxShadow: "0 -4px 20px rgba(0,0,0,0.1)",
          display: "flex",
          flexDirection: "column",
          alignItems: "center",
          gap: 14,
          position: "relative",
          zIndex: 2
        }}>
          {/* Mic button ·ªü gi·ªØa - click ƒë·ªÉ b·∫Øt ƒë·∫ßu/d·ª´ng recording */}
          <button
            ref={micButtonRef}
            onClick={() => {
              if (sending) return; // Kh√¥ng l√†m g√¨ n·∫øu ƒëang g·ª≠i
              
              // D·ª´ng TTS n·∫øu ƒëang ph√°t (cho ph√©p c·∫Øt ngang)
              if (playingAudio) {
                window.speechSynthesis.cancel();
                setPlayingAudio(null);
              }
              
              // N·∫øu ƒëang recording, d·ª´ng v√† g·ª≠i
              if (isRecording) {
                stopRecording();
                return;
              }
              
              // N·∫øu ch∆∞a recording, b·∫Øt ƒë·∫ßu ghi √¢m
              startRecording();
            }}
            disabled={sending}
            style={{
              width: 80,
              height: 80,
              borderRadius: "50%",
              border: "none",
              background: isRecording 
                ? "linear-gradient(135deg, #ef4444 0%, #dc2626 100%)"
                : "linear-gradient(135deg, #667eea 0%, #764ba2 100%)",
              color: "white",
              cursor: sending ? "not-allowed" : "pointer",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              boxShadow: isRecording 
                ? "0 0 20px rgba(239, 68, 68, 0.6)"
                : "0 4px 15px rgba(102, 126, 234, 0.4)",
              transition: "all 0.3s",
              transform: isRecording ? "scale(1.1)" : "scale(1)",
              zIndex: 10
            }}
          >
            <FaMicrophone size={32} />
          </button>
          
          {/* Status text */}
          <div style={{ textAlign: "center" }}>
            <p style={{ 
              margin: 0, 
              color: "#666", 
              fontSize: 14,
              fontWeight: 500
            }}>
              {isRecording 
                ? "ƒêang ghi √¢m... (T·ª± ƒë·ªông g·ª≠i khi im l·∫∑ng 2 gi√¢y ho·∫∑c b·∫•m mic ƒë·ªÉ d·ª´ng v√† g·ª≠i)" 
                : playingAudio
                ? "AI ƒëang n√≥i... (·∫§n v√†o mic ƒë·ªÉ c·∫Øt ngang v√† n√≥i)"
                : "Nh·∫•n v√†o mic ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi √¢m"}
            </p>
          </div>
        </div>
      </div>

      <style>{`
        @keyframes fadeIn {
          from {
            opacity: 0;
            transform: translateY(10px);
          }
          to {
            opacity: 1;
            transform: translateY(0);
          }
        }
        @keyframes pulse {
          0%, 100% {
            opacity: 1;
          }
          50% {
            opacity: 0.5;
          }
        }
      `}</style>
    </div>
  );
}
